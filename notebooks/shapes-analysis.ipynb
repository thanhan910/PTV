{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyptvgtfs\n",
    "from pyptvgtfs import BRANCH_IDS, GTFS_FILE_FIELDS_TYPES, TABLE_NAMES, BRANCH_IDS_ALL\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "import re\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point, LineString, MultiLineString, Polygon, MultiPolygon, GeometryCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSIONS = [\n",
    "    '20220403_025040',\n",
    "    '20230805_030129',\n",
    "    '20231021_105623',\n",
    "    '20240229_224711'\n",
    "]\n",
    "\n",
    "# VERSIONS x BRANCHES\n",
    "VERSIONS_BRANCHES = [(v, b) for v in VERSIONS for b in BRANCH_IDS]\n",
    "VERSIONS_BRANCHES_ALL = [(v, b) for v in VERSIONS for b in BRANCH_IDS_ALL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFS_LIST = [pyptvgtfs.process_gtfs_zip(f'../downloads/{f}/gtfs.zip', f) for f in VERSIONS]\n",
    "# Per file: 40s - 1m - 3m. 5 files: 2m - 5m. 7 files: 3m - 5sm. 2 files: 1m 30s - 2m\n",
    "\n",
    "DFK : dict[tuple, pd.DataFrame] = pd.concat(DFS_LIST, axis=0).set_index(['version_id', 'branch_id', 'table_name'])['df'].to_dict()\n",
    "\n",
    "DF : dict[str, dict[str, dict[str, pd.DataFrame]]] = {}\n",
    "for (vid, bid, table_name), df in DFK.items():\n",
    "    DF[vid] = DF.get(vid, {})\n",
    "    DF[vid][bid] = DF[vid].get(bid, {})\n",
    "    DF[vid][bid][table_name] = df\n",
    "\n",
    "\n",
    "# Either route_code_extra or route_no is unique, and the other denotes the variants of the same route\n",
    "# If both are not unique, then the route_no is the variant of the route, while the route_code_extra should also belongs to route_code\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    df_route_idx = DF[vid][bid]['route_ids']['route_id'].apply(lambda x: x.split('-'))\n",
    "    df_route_code_extra = df_route_idx.apply(lambda x: x[2] if len(x) >= 5 else '')\n",
    "    df_route_no = df_route_idx.apply(lambda x: x[-1])\n",
    "    if not (df_route_code_extra.nunique() == 1 or df_route_no.nunique() == 1):\n",
    "        assert bid == '4'\n",
    "        \n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['service_ids'] = pd.DataFrame(DF[vid][bid]['calendar']['service_id'].drop_duplicates().reset_index(drop=True))\n",
    "    DF[vid][bid]['route_ids'] = pd.DataFrame(DF[vid][bid]['routes']['route_id'].drop_duplicates().reset_index(drop=True))\n",
    "    DF[vid][bid]['trip_ids'] = pd.DataFrame(DF[vid][bid]['trips']['trip_id'].drop_duplicates().reset_index(drop=True))\n",
    "    DF[vid][bid]['shape_ids'] = pd.DataFrame(DF[vid][bid]['shapes']['shape_id'].drop_duplicates().reset_index(drop=True))\n",
    "# 1s - 2s\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    # Get all types of delimiters\n",
    "    DF[vid][bid]['patterns'] = {}\n",
    "    DF[vid][bid]['patterns'] = {}\n",
    "    DF[vid][bid]['delimiters'] = {}\n",
    "    for id_name in ['service_id', 'route_id', 'trip_id', 'shape_id']:\n",
    "        id_pattern = DF[vid][bid][f'{id_name}s'][id_name].str.replace(r'[a-zA-Z0-9]+', '0', regex=True).drop_duplicates()\n",
    "        DF[vid][bid]['patterns'][id_name] = id_pattern.unique()\n",
    "        id_pattern = id_pattern.str.replace(r'[0]', '', regex=True).unique()\n",
    "        # Sum all in DF[vid][bid]['patterns'][id_name] and remove duplicates\n",
    "        DF[vid][bid]['delimiters'][id_name] = set(''.join(id_pattern))\n",
    "# 2s - 5s\n",
    "        \n",
    "ID_PATTERNS = {\n",
    "    k: pd.DataFrame(\n",
    "        data=[(vid, bid, DF[vid][bid][\"patterns\"][k]) for vid, bid in VERSIONS_BRANCHES],\n",
    "        columns=[\"version_id\", \"branch_id\", \"pattern\"],\n",
    "    )\n",
    "    .explode(\"pattern\")\n",
    "    .groupby(\"pattern\")[\"branch_id\"]\n",
    "    .apply(lambda x: sorted(int(i) for i in x.unique()))\n",
    "    .to_dict()\n",
    "    for k in [\"service_id\", \"route_id\", \"trip_id\", \"shape_id\"]\n",
    "}\n",
    "\n",
    "\n",
    "ID_PATTERNS == {\n",
    "    \"service_id\": {\n",
    "        \"0\": [1, 2, 3, 4, 5, 6, 10, 11],\n",
    "        \"0+0\": [1, 2, 3, 4, 5, 6, 10],\n",
    "        \"0+0_0\": [1, 2, 3, 4, 5, 6, 10],\n",
    "        \"0-0-0-0\": [4],\n",
    "        \"0-0-0-0-0\": [4],\n",
    "        \"0_0\": [1, 2, 3, 4, 5, 6, 10],\n",
    "    },\n",
    "    \"route_id\": {\n",
    "        \"0-0-0-0\": [1, 2, 3, 4, 5, 6, 10, 11],\n",
    "        \"0-0-0-0-0\": [1, 2, 3, 4, 5, 6, 10],\n",
    "    },\n",
    "    \"trip_id\": {\n",
    "        \"0-0--0-0-0\": [4],\n",
    "        \"0-0-0-0-0-0\": [4],\n",
    "        \"0.0.0-0-0-0-0.0.0\": [1, 2, 3, 5, 6, 10],\n",
    "        \"0.0.0-0-0-0.0.0\": [1, 2, 3, 5, 6, 10, 11],\n",
    "    },\n",
    "    \"shape_id\": {\n",
    "        \"0-0-0-0-0.0.0\": [1, 2, 3, 4, 5, 6, 10],\n",
    "        \"0-0-0-0.0.0\": [1, 2, 3, 4, 5, 6, 10, 11],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['service_ids']['service_class'] = DF[vid][bid]['service_ids']['service_id'].apply(lambda x: x.split('-')[0].split('+')[0].split('_')[0])\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    df_route_idx = DF[vid][bid]['route_ids']['route_id'].apply(lambda x: x.split('-'))\n",
    "    DF[vid][bid]['route_ids']['route_code'] = df_route_idx.apply(lambda x: x[1] + x[2] if (bid == '4' and len(x) >= 5) else x[1])\n",
    "    DF[vid][bid]['route_ids']['route_no'] = df_route_idx.apply(lambda x: x[-1])\n",
    "    DF[vid][bid]['route_ids']['branch'] = df_route_idx.apply(lambda x: x[0])\n",
    "    DF[vid][bid]['route_ids']['range'] = df_route_idx.apply(lambda x: x[-2])\n",
    "\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    df_shape_idx = DF[vid][bid]['shape_ids']['shape_id'].apply(lambda x: x.split('.'))\n",
    "    df_route_id = df_shape_idx.apply(lambda x: x[0])\n",
    "    df_route_idx = df_route_id.apply(lambda x: x.split('-'))\n",
    "\n",
    "    DF[vid][bid]['shape_ids']['route_id'] = df_route_id\n",
    "    DF[vid][bid]['shape_ids']['route_code'] = df_route_idx.apply(lambda x: x[1] + x[2] if (bid == '4' and len(x) >= 5) else x[1])\n",
    "    DF[vid][bid]['shape_ids']['route_no'] = df_route_idx.apply(lambda x: x[-1])\n",
    "    DF[vid][bid]['shape_ids']['branch'] = df_route_idx.apply(lambda x: x[0])\n",
    "    DF[vid][bid]['shape_ids']['direction'] = df_shape_idx.apply(lambda x: x[2])\n",
    "    DF[vid][bid]['shape_ids']['range'] = df_route_idx.apply(lambda x: x[-2])\n",
    "    DF[vid][bid]['shape_ids']['shape_no'] = df_shape_idx.apply(lambda x: x[1])\n",
    "# 1s\n",
    "    \n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    if bid == '4':\n",
    "        df_trip_idx = DF[vid][bid]['trip_ids']['trip_id'].apply(lambda x: x.split('-'))\n",
    "        DF[vid][bid]['trip_ids']['route_code'] = df_route_idx.apply(lambda x: x[1] + x[2] if (bid == '4' and len(x) >= 5) else x[1])\n",
    "        DF[vid][bid]['trip_ids']['route_no'] = df_trip_idx.apply(lambda x: x[3])\n",
    "        DF[vid][bid]['trip_ids']['branch'] = df_trip_idx.apply(lambda x: x[0])\n",
    "        DF[vid][bid]['trip_ids']['service_class'] = df_trip_idx.apply(lambda x: x[4])\n",
    "        DF[vid][bid]['trip_ids']['trip_no'] = df_trip_idx.apply(lambda x: x[5])\n",
    "    else:\n",
    "        df_trip_idx = DF[vid][bid]['trip_ids']['trip_id'].apply(lambda x: x.split('.'))\n",
    "        df_route_id = df_trip_idx.apply(lambda x: x[2])\n",
    "        df_route_idx = df_route_id.apply(lambda x: x.split('-'))\n",
    "\n",
    "        # DF[vid][bid]['trip_ids']['route_id'] = df_route_id\n",
    "        # DF[vid][bid]['trip_ids']['direction'] = df_trip_idx.apply(lambda x: x[4])\n",
    "        # DF[vid][bid]['trip_ids']['shape_no'] = df_trip_idx.apply(lambda x: x[3])\n",
    "        # DF[vid][bid]['trip_ids']['range'] = df_route_idx.apply(lambda x: x[-2])\n",
    "        \n",
    "        DF[vid][bid]['trip_ids']['route_code'] = df_route_idx.apply(lambda x: x[1] + x[2] if (bid == '4' and len(x) >= 5) else x[1])\n",
    "        DF[vid][bid]['trip_ids']['route_no'] = df_route_idx.apply(lambda x: x[-1])\n",
    "        DF[vid][bid]['trip_ids']['branch'] = df_route_idx.apply(lambda x: x[0])\n",
    "        DF[vid][bid]['trip_ids']['service_class'] = df_trip_idx.apply(lambda x: x[1])\n",
    "        DF[vid][bid]['trip_ids']['trip_no'] = df_trip_idx.apply(lambda x: x[0])\n",
    "# 7s - 20s\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['shapes'].sort_values(by=['shape_id', 'shape_pt_sequence'], inplace=True)\n",
    "# 20s - 30s\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:    \n",
    "    DF[vid][bid]['shapes']['point'] = list(zip(DF[vid][bid]['shapes']['shape_pt_lon'], DF[vid][bid]['shapes']['shape_pt_lat']))\n",
    "    # 1m - 2m. Occasionally 20s - 30s\n",
    "\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['lines'] = DF[vid][bid]['shapes'].groupby('shape_id')[['shape_pt_sequence', 'point']].aggregate({'shape_pt_sequence': list, 'point': list}).reset_index()\n",
    "    # 2m 30s. Occasionally 1m - 2m\n",
    "\n",
    "# Optional: drop column\n",
    "# for vid, bid in VERSIONS_BRANCHES:    \n",
    "#     DF[vid][bid]['shapes'].drop(columns=['points'], inplace=True)\n",
    "#     # 1m - 2m\n",
    "    \n",
    "# Maybe faster:\n",
    "# for vid, bid in VERSIONS_BRANCHES:\n",
    "#     DF[vid][bid]['lines'] = DF[vid][bid]['shapes'].groupby('shape_id')[['shape_pt_lon', 'shape_pt_lat']].apply(lambda x: list(zip(x['shape_pt_lon'], x['shape_pt_lat']))).reset_index(name='line')\n",
    "#     # 2m - 3m\n",
    "\n",
    "# Proof that all shape_pt_sequence are continuous\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    assert DF[vid][bid]['lines']['shape_pt_sequence'].apply(lambda x: x[-1] - x[0] + 1 == len(x)).all(), (vid, bid)\n",
    "    # 1s - 2s\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['lines'] = pd.merge(DF[vid][bid]['lines'], DF[vid][bid]['shape_ids'], on='shape_id')\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['lines']['geometry'] = DF[vid][bid]['lines']['point'].apply(lambda x: LineString(x))\n",
    "    # 3m 30s - 5m\n",
    "\n",
    "\n",
    "# Total from start: 13m - 22m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(monday, tuesday, wednesday, thursday, friday, saturday, sunday, start_date, end_date):\n",
    "    # Get list of dates based on week pattern and date range\n",
    "    week_pattern = [bool(int(monday)), bool(int(tuesday)), bool(int(wednesday)), bool(int(thursday)), bool(int(friday)), bool(int(saturday)), bool(int(sunday))]\n",
    "    start_date = pd.to_datetime(start_date, format='%Y%m%d')\n",
    "    end_date = pd.to_datetime(end_date, format='%Y%m%d')\n",
    "    dates = pd.date_range(start_date, end_date)\n",
    "    return dates[[week_pattern[i] for i in dates.dayofweek]]\n",
    "    \n",
    "def get_dates_df_calendar(df_calendar: pd.DataFrame, df_calendar_dates: pd.DataFrame):\n",
    "    \n",
    "    weekdate_columns = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    daterange_columns = ['start_date', 'end_date']\n",
    "    pattern_columns = weekdate_columns + daterange_columns\n",
    "    \n",
    "    # Drop duplicates to reduce the number of rows to be processed\n",
    "    df_dates = df_calendar[pattern_columns].drop_duplicates()\n",
    "    \n",
    "    # Get date list based on week pattern and date range\n",
    "    df_dates['date'] = df_dates.apply(lambda x: get_dates(x['monday'], x['tuesday'], x['wednesday'], x['thursday'], x['friday'], x['saturday'], x['sunday'], x['start_date'], x['end_date']), axis=1)\n",
    "\n",
    "    df_dates['date'] = df_dates['date'].apply(lambda x: [y.strftime('%Y%m%d') for y in x])\n",
    "    \n",
    "    # Join the date list with the original calendar table\n",
    "    df_dates = pd.merge(df_calendar, df_dates, on=pattern_columns, how='left')\n",
    "    \n",
    "    # Explode the date list into separate rows\n",
    "    df_dates = df_dates[['service_id', 'date']].explode('date')\n",
    "\n",
    "    # Join the date df with the calendar_dates df\n",
    "    df_dates = pd.merge(df_dates, df_calendar_dates.astype({'date': str, 'exception_type': str}), on=['service_id', 'date'], how='outer')\n",
    "    \n",
    "    # Drop 2 and keep 1 and NaN\n",
    "    df_dates = df_dates[df_dates['exception_type'] != '2'].reset_index(drop=True)\n",
    "\n",
    "    return df_dates\n",
    "\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['dates'] = get_dates_df_calendar(DF[vid][bid]['calendar'], DF[vid][bid]['calendar_dates'])\n",
    "# 1s - 5s\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['route_services'] = DF[vid][bid]['trips'][['route_id', 'service_id']].drop_duplicates().reset_index(drop=True)\n",
    "    DF[vid][bid]['route_services'] = pd.merge(DF[vid][bid]['route_services'], DF[vid][bid]['route_ids'], on='route_id', how='left')\n",
    "    DF[vid][bid]['route_services'] = pd.merge(DF[vid][bid]['route_services'], DF[vid][bid]['service_ids'], on='service_id', how='left')\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['route_service_dates'] = pd.merge(DF[vid][bid]['route_services'], DF[vid][bid]['dates'], on='service_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems that there is no significant difference in shapes among different branches for each route_code. In each route, the shape with the most number of points among each branch is the same.\n",
    "route_code = '742'\n",
    "branch_codes = [19, 21, 22, 29]\n",
    "branch_codes = [str(i) for i in branch_codes]\n",
    "df903 = DF['20240229_224711']['4']['lines'][(DF['20240229_224711']['4']['lines']['route_code'] == route_code)].sort_values(by='point', key=lambda x: x.apply(lambda x: len(x)), ascending=False)\n",
    "tl = {t:df903[df903['branch'] == t]['geometry'].iloc[0] for t in branch_codes}\n",
    "for l in tl.values():\n",
    "    plt.plot(*l.xy, linewidth=1)\n",
    "plt.legend(branch_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['rd'] = DF[vid][bid]['route_service_dates'].groupby(['route_code', 'date'])['branch'].unique()\n",
    "    # 1m 30s - 2m - 3m\n",
    "\n",
    "# route_code-date pair may still have different branches\n",
    "for vid in VERSIONS:\n",
    "    print(DF[vid]['4']['rd'].apply(lambda x: len(x)).unique())\n",
    "\n",
    "dfrd4 = DF[vid]['4']['route_service_dates'].copy(deep=True)\n",
    "dfrd4['service_class_branch'] = dfrd4['service_class'] + '-' + dfrd4['branch'].astype(str)\n",
    "dfrd4 = dfrd4.groupby(['route_code', 'date'])['service_class_branch'].unique().reset_index(name='service_class_branches')\n",
    "dfrd4['len'] = dfrd4['service_class_branches'].apply(len)\n",
    "dfrd4.sort_values(by='len', ascending=False, inplace=True)\n",
    "\n",
    "dfsb4 = DF[vid]['4']['route_service_dates'][['service_class', 'branch']].drop_duplicates()\n",
    "dfsb4 = dfsb4.groupby('branch')['service_class'].unique().reset_index(name='service_classes')\n",
    "\n",
    "dfsb4['pattern'] = dfsb4['service_classes'].apply(lambda x: '-'.join(sorted(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_trips = DF[vid]['4']['stop_times'].sort_values(by=['trip_id', 'stop_sequence'])\n",
    "# 5s - 10s\n",
    "df4_trips1 = df4_trips.groupby('trip_id')['stop_id'].apply(np.array).reset_index(name='stop_ids')\n",
    "# 2s - 5s, using np.array. Faster than using list (2s - 5s)\n",
    "df4_trips2 = df4_trips.groupby('trip_id')['stop_sequence'].apply(np.array).reset_index(name='stops_sequence')\n",
    "# 3s - 5s, using np.array. Faster than using list (1m 30s - 4m)\n",
    "df4_trips = pd.merge(df4_trips1, df4_trips2, on='trip_id')\n",
    "df4_trips = pd.merge(df4_trips, DF[vid]['4']['trips'][['trip_id', 'direction_id']], on='trip_id', how='left')\n",
    "df4_trips = pd.merge(df4_trips, DF[vid]['4']['trip_ids'], on='trip_id', how='left')\n",
    "df4_trips['pattern'] = df4_trips['stop_ids'].apply(lambda x: '-'.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_stop_ids_sequence(stop_ids_list):\n",
    "    stop_ids_max : list = max(stop_ids_list, key=lambda x: len(x)).copy()\n",
    "    for stop_ids in stop_ids_list:\n",
    "        old_i = 0\n",
    "        stack = []\n",
    "        for stop_id in stop_ids:\n",
    "            cur_i = old_i\n",
    "            while cur_i < len(stop_ids_max) and stop_id != stop_ids_max[cur_i]:\n",
    "                cur_i += 1\n",
    "            if cur_i >= len(stop_ids_max):\n",
    "                stack.append(stop_id)\n",
    "            else:\n",
    "                stack.extend(stop_ids_max[old_i:cur_i+1])\n",
    "                old_i = cur_i+1\n",
    "        stack.extend(stop_ids_max[old_i:])\n",
    "        stop_ids_max = stack\n",
    "    return stop_ids_max\n",
    "\n",
    "def get_true_stop_order_sequence(stop_ids_full: list, stop_ids: list, stops_sequence: list[int]):\n",
    "    # assert len(stop_id) == len(stop_sequence)\n",
    "    stop_true_sequence = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < len(stop_ids_full) and j < len(stop_ids):\n",
    "        if stop_ids_full[i] == stop_ids[j]:\n",
    "            new_ix = i + 1\n",
    "            cur_ix = stops_sequence[j]\n",
    "            if new_ix < cur_ix:\n",
    "                stop_true_sequence.append(cur_ix)\n",
    "                i = cur_ix - 1\n",
    "            else:\n",
    "                stop_true_sequence.append(new_ix)\n",
    "            j += 1\n",
    "        i += 1\n",
    "    assert j == len(stop_ids)\n",
    "    return stop_true_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_tripstops_full_all_patterns = df4_trips.drop_duplicates(subset=['route_code', 'direction_id', 'pattern'])\n",
    "df4_tripstops_full = df4_tripstops_full_all_patterns.groupby(['route_code', 'direction_id'])['stop_ids'].apply(np.array)\n",
    "df4_tripstops_full = df4_tripstops_full.apply(lambda x: merge_stop_ids_sequence(x))\n",
    "df4_tripstops_full.rename('stop_ids_full', inplace=True)\n",
    "df4_tripstops_full = df4_tripstops_full.reset_index()\n",
    "df4_tripstops_full['stop_ids_full_count'] = df4_tripstops_full['stop_ids_full'].apply(len)\n",
    "df4_trips_rck = pd.merge(df4_trips, df4_tripstops_full, on=['route_code', 'direction_id'], how='left')\n",
    "df4_tripstops_full_all_patterns = pd.merge(df4_tripstops_full_all_patterns, df4_tripstops_full, on=['route_code', 'direction_id'], how='left')\n",
    "df4_tripstops_full_all_patterns['stops_sequence_full'] = df4_tripstops_full_all_patterns.apply(lambda x: get_true_stop_order_sequence(x['stop_ids_full'], x['stop_ids'], x['stops_sequence']), axis=1)\n",
    "df4_tripstops_full_all_patterns = df4_tripstops_full_all_patterns[['route_code', 'direction_id', 'pattern', 'stop_ids_full', 'stop_ids_full_count', 'stops_sequence_full']] \n",
    "df4_trips_2 = pd.merge(df4_trips, df4_tripstops_full_all_patterns, on=['route_code', 'direction_id', 'pattern'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_code</th>\n",
       "      <th>direction_id</th>\n",
       "      <th>stop_ids_full</th>\n",
       "      <th>stop_ids_full_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>[46700, 50137, 50249, 50274, 50247, 50245, 443...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>[28543, 50205, 50203, 22201, 50201, 50093, 500...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>[46700, 50137, 50135, 49061, 50057, 50059, 443...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>[28537, 50205, 50203, 21656, 21658, 21661, 216...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>[28539, 47733, 50068, 50066, 27987, 27989, 279...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>979</td>\n",
       "      <td>1</td>\n",
       "      <td>[19818, 45457, 45458, 4258, 4260, 4261, 4262, ...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>981</td>\n",
       "      <td>0</td>\n",
       "      <td>[19821, 3657, 3658, 3659, 3660, 3661, 3662, 36...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>981</td>\n",
       "      <td>1</td>\n",
       "      <td>[18112, 45576, 51217, 27962, 47236, 47237, 512...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>982</td>\n",
       "      <td>0</td>\n",
       "      <td>[19820, 39613, 1311, 1312, 1313, 1314, 1315, 1...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>982</td>\n",
       "      <td>1</td>\n",
       "      <td>[16342, 16344, 21405, 21404, 46540, 21402, 513...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>713 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    route_code direction_id  \\\n",
       "0          150            0   \n",
       "1          150            1   \n",
       "2          151            0   \n",
       "3          151            1   \n",
       "4          152            0   \n",
       "..         ...          ...   \n",
       "708        979            1   \n",
       "709        981            0   \n",
       "710        981            1   \n",
       "711        982            0   \n",
       "712        982            1   \n",
       "\n",
       "                                         stop_ids_full  stop_ids_full_count  \n",
       "0    [46700, 50137, 50249, 50274, 50247, 50245, 443...                   22  \n",
       "1    [28543, 50205, 50203, 22201, 50201, 50093, 500...                   23  \n",
       "2    [46700, 50137, 50135, 49061, 50057, 50059, 443...                   21  \n",
       "3    [28537, 50205, 50203, 21656, 21658, 21661, 216...                   24  \n",
       "4    [28539, 47733, 50068, 50066, 27987, 27989, 279...                   24  \n",
       "..                                                 ...                  ...  \n",
       "708  [19818, 45457, 45458, 4258, 4260, 4261, 4262, ...                   46  \n",
       "709  [19821, 3657, 3658, 3659, 3660, 3661, 3662, 36...                   58  \n",
       "710  [18112, 45576, 51217, 27962, 47236, 47237, 512...                   59  \n",
       "711  [19820, 39613, 1311, 1312, 1313, 1314, 1315, 1...                   61  \n",
       "712  [16342, 16344, 21405, 21404, 46540, 21402, 513...                   58  \n",
       "\n",
       "[713 rows x 4 columns]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4_tripstops_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df4_trips_2.apply(lambda x: len(x['stops_sequence']) == len(x['stops_sequence_full']), axis=1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_trips_3 = df4_trips_2.explode(['stop_ids', 'stops_sequence', 'stops_sequence_full'])[['trip_id', 'route_code', 'direction_id', 'stop_ids', 'stops_sequence', 'stops_sequence_full']]\n",
    "# 5s - 10s\n",
    "df4_trips_3.rename(columns={'stop_ids': 'stop_id', 'stops_sequence': 'stop_sequence', 'stops_sequence_full': 'stop_sequence_real'}, inplace=True)\n",
    "df4_trips_3 = pd.merge(df4_trips_3, DF[vid]['4']['stop_times'], on=['trip_id', 'stop_id', 'stop_sequence'], how='left')\n",
    "# 10s - 20s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = df4_trips_3[(df4_trips_3['route_code'] == '742') & (df4_trips_3['direction_id'] == '0')].sort_values(by=['trip_id', 'stop_sequence_real'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 230. MiB for an array with shape (1063, 28383) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[337], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf4_trips_3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroute_code\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdirection_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstop_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstop_sequence_real\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrip_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marrival_time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\An\\Documents\\GitHub\\PTV\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1824\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1826\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[0;32m   1827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1829\u001b[0m         ):\n\u001b[0;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1836\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\An\\Documents\\GitHub\\PTV\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1889\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m     not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n\u001b[1;32m-> 1889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_applied_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnot_indexed_same\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1894\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\An\\Documents\\GitHub\\PTV\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:1584\u001b[0m, in \u001b[0;36mDataFrameGroupBy._wrap_applied_output\u001b[1;34m(self, data, values, not_indexed_same, is_transform)\u001b[0m\n\u001b[0;32m   1582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_constructor()\n\u001b[0;32m   1583\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_not_none, DataFrame):\n\u001b[1;32m-> 1584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concat_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnot_indexed_same\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnot_indexed_same\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1590\u001b[0m key_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mresult_index \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_not_none, (np\u001b[38;5;241m.\u001b[39mndarray, Index)):\n\u001b[0;32m   1593\u001b[0m     \u001b[38;5;66;03m# GH#1738: values is list of arrays of unequal lengths\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;66;03m#  fall through to the outer else clause\u001b[39;00m\n\u001b[0;32m   1595\u001b[0m     \u001b[38;5;66;03m# TODO: sure this is right?  we used to do this\u001b[39;00m\n\u001b[0;32m   1596\u001b[0m     \u001b[38;5;66;03m#  after raising AttributeError above\u001b[39;00m\n\u001b[0;32m   1597\u001b[0m     \u001b[38;5;66;03m# GH 18930\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\An\\Documents\\GitHub\\PTV\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1457\u001b[0m, in \u001b[0;36mGroupBy._concat_objects\u001b[1;34m(self, values, not_indexed_same, is_transform)\u001b[0m\n\u001b[0;32m   1454\u001b[0m     group_levels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mlevels\n\u001b[0;32m   1455\u001b[0m     group_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mnames\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1466\u001b[0m     \u001b[38;5;66;03m# GH5610, returns a MI, with the first level being a\u001b[39;00m\n\u001b[0;32m   1467\u001b[0m     \u001b[38;5;66;03m# range index\u001b[39;00m\n\u001b[0;32m   1468\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(values)))\n",
      "File \u001b[1;32mc:\\Users\\An\\Documents\\GitHub\\PTV\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\An\\Documents\\GitHub\\PTV\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\An\\Documents\\GitHub\\PTV\\venv\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:189\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    187\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m values\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_concatenate_join_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fastpath:\n",
      "File \u001b[1;32mc:\\Users\\An\\Documents\\GitHub\\PTV\\venv\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:486\u001b[0m, in \u001b[0;36m_concatenate_join_units\u001b[1;34m(join_units, copy)\u001b[0m\n\u001b[0;32m    483\u001b[0m     concat_values \u001b[38;5;241m=\u001b[39m ensure_block_shape(concat_values, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     concat_values \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m empty_dtype \u001b[38;5;241m!=\u001b[39m empty_dtype_future:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m empty_dtype \u001b[38;5;241m==\u001b[39m concat_values\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;66;03m# GH#39122, GH#40893\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\An\\Documents\\GitHub\\PTV\\venv\\Lib\\site-packages\\pandas\\core\\dtypes\\concat.py:78\u001b[0m, in \u001b[0;36mconcat_compat\u001b[1;34m(to_concat, axis, ea_compat_axis)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     77\u001b[0m     to_concat_arrs \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence[np.ndarray]\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_concat)\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat_arrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m to_concat_eas \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence[ExtensionArray]\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_concat)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ea_compat_axis:\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# We have 1D objects, that don't support axis keyword\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 230. MiB for an array with shape (1063, 28383) and data type object"
     ]
    }
   ],
   "source": [
    "df4_trips_3.groupby(['route_code', 'direction_id']).apply(lambda x: x.pivot_table(index=['stop_id', 'stop_sequence_real'], columns='trip_id', values='arrival_time', aggfunc='first').sort_index(level=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>19-742--1-MF3-31</th>\n",
       "      <th>19-742--1-MF3-42</th>\n",
       "      <th>19-742--1-MF3-46</th>\n",
       "      <th>19-742--1-MF3-48</th>\n",
       "      <th>19-742--1-MF3-50</th>\n",
       "      <th>19-742--1-MF3-52</th>\n",
       "      <th>19-742--1-MF3-54</th>\n",
       "      <th>19-742--1-MF3-56</th>\n",
       "      <th>19-742--1-MF3-58</th>\n",
       "      <th>19-742--1-MF3-60</th>\n",
       "      <th>...</th>\n",
       "      <th>29-742--1-Sun1-48</th>\n",
       "      <th>29-742--1-Sun1-49</th>\n",
       "      <th>29-742--1-Sun2-1</th>\n",
       "      <th>29-742--1-Sun2-47</th>\n",
       "      <th>29-742--1-Sun2-48</th>\n",
       "      <th>29-742--1-Sun2-49</th>\n",
       "      <th>29-742--1-Sun3-12</th>\n",
       "      <th>29-742--1-Sun3-15</th>\n",
       "      <th>29-742--1-Sun3-5</th>\n",
       "      <th>29-742--1-Sun4-5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop_id</th>\n",
       "      <th>stop_sequence_real</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21270</th>\n",
       "      <th>1</th>\n",
       "      <td>16:51:00</td>\n",
       "      <td>06:44:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07:32:00</td>\n",
       "      <td>08:46:00</td>\n",
       "      <td>09:15:00</td>\n",
       "      <td>11:48:00</td>\n",
       "      <td>12:48:00</td>\n",
       "      <td>13:18:00</td>\n",
       "      <td>14:17:00</td>\n",
       "      <td>...</td>\n",
       "      <td>13:24:00</td>\n",
       "      <td>16:24:00</td>\n",
       "      <td>08:50:00</td>\n",
       "      <td>11:25:00</td>\n",
       "      <td>13:24:00</td>\n",
       "      <td>16:24:00</td>\n",
       "      <td>13:39:00</td>\n",
       "      <td>14:16:00</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>12:24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47093</th>\n",
       "      <th>2</th>\n",
       "      <td>16:51:00</td>\n",
       "      <td>06:44:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07:32:00</td>\n",
       "      <td>08:46:00</td>\n",
       "      <td>09:15:00</td>\n",
       "      <td>11:48:00</td>\n",
       "      <td>12:48:00</td>\n",
       "      <td>13:18:00</td>\n",
       "      <td>14:17:00</td>\n",
       "      <td>...</td>\n",
       "      <td>13:24:00</td>\n",
       "      <td>16:24:00</td>\n",
       "      <td>08:50:00</td>\n",
       "      <td>11:25:00</td>\n",
       "      <td>13:24:00</td>\n",
       "      <td>16:24:00</td>\n",
       "      <td>13:39:00</td>\n",
       "      <td>14:16:00</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>12:24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15715</th>\n",
       "      <th>3</th>\n",
       "      <td>16:51:00</td>\n",
       "      <td>06:44:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07:32:00</td>\n",
       "      <td>08:46:00</td>\n",
       "      <td>09:15:00</td>\n",
       "      <td>11:48:00</td>\n",
       "      <td>12:48:00</td>\n",
       "      <td>13:18:00</td>\n",
       "      <td>14:17:00</td>\n",
       "      <td>...</td>\n",
       "      <td>13:24:00</td>\n",
       "      <td>16:24:00</td>\n",
       "      <td>08:50:00</td>\n",
       "      <td>11:25:00</td>\n",
       "      <td>13:24:00</td>\n",
       "      <td>16:24:00</td>\n",
       "      <td>13:39:00</td>\n",
       "      <td>14:16:00</td>\n",
       "      <td>09:00:00</td>\n",
       "      <td>12:24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15716</th>\n",
       "      <th>4</th>\n",
       "      <td>16:53:00</td>\n",
       "      <td>06:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07:33:00</td>\n",
       "      <td>08:48:00</td>\n",
       "      <td>09:17:00</td>\n",
       "      <td>11:49:00</td>\n",
       "      <td>12:49:00</td>\n",
       "      <td>13:19:00</td>\n",
       "      <td>14:18:00</td>\n",
       "      <td>...</td>\n",
       "      <td>13:25:00</td>\n",
       "      <td>16:25:00</td>\n",
       "      <td>08:51:00</td>\n",
       "      <td>11:26:00</td>\n",
       "      <td>13:25:00</td>\n",
       "      <td>16:25:00</td>\n",
       "      <td>13:41:00</td>\n",
       "      <td>14:18:00</td>\n",
       "      <td>09:01:00</td>\n",
       "      <td>12:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15717</th>\n",
       "      <th>5</th>\n",
       "      <td>16:53:00</td>\n",
       "      <td>06:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07:33:00</td>\n",
       "      <td>08:48:00</td>\n",
       "      <td>09:17:00</td>\n",
       "      <td>11:49:00</td>\n",
       "      <td>12:49:00</td>\n",
       "      <td>13:19:00</td>\n",
       "      <td>14:18:00</td>\n",
       "      <td>...</td>\n",
       "      <td>13:25:00</td>\n",
       "      <td>16:25:00</td>\n",
       "      <td>08:51:00</td>\n",
       "      <td>11:26:00</td>\n",
       "      <td>13:25:00</td>\n",
       "      <td>16:25:00</td>\n",
       "      <td>13:41:00</td>\n",
       "      <td>14:18:00</td>\n",
       "      <td>09:01:00</td>\n",
       "      <td>12:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47928</th>\n",
       "      <th>79</th>\n",
       "      <td>18:12:00</td>\n",
       "      <td>07:59:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09:04:00</td>\n",
       "      <td>10:05:00</td>\n",
       "      <td>10:34:00</td>\n",
       "      <td>13:01:00</td>\n",
       "      <td>14:02:00</td>\n",
       "      <td>14:32:00</td>\n",
       "      <td>15:33:00</td>\n",
       "      <td>...</td>\n",
       "      <td>14:37:00</td>\n",
       "      <td>17:38:00</td>\n",
       "      <td>10:03:00</td>\n",
       "      <td>12:40:00</td>\n",
       "      <td>14:37:00</td>\n",
       "      <td>17:38:00</td>\n",
       "      <td>14:55:00</td>\n",
       "      <td>15:31:00</td>\n",
       "      <td>10:15:00</td>\n",
       "      <td>13:38:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11527</th>\n",
       "      <th>80</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <th>81</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <th>82</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07:32:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19800</th>\n",
       "      <th>83</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07:40:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows Ã— 246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "trip_id                    19-742--1-MF3-31 19-742--1-MF3-42 19-742--1-MF3-46  \\\n",
       "stop_id stop_sequence_real                                                      \n",
       "21270   1                          16:51:00         06:44:00              NaN   \n",
       "47093   2                          16:51:00         06:44:00              NaN   \n",
       "15715   3                          16:51:00         06:44:00              NaN   \n",
       "15716   4                          16:53:00         06:45:00              NaN   \n",
       "15717   5                          16:53:00         06:45:00              NaN   \n",
       "...                                     ...              ...              ...   \n",
       "47928   79                         18:12:00         07:59:00              NaN   \n",
       "11527   80                              NaN              NaN         07:30:00   \n",
       "1972    81                              NaN              NaN         07:30:00   \n",
       "1560    82                              NaN              NaN         07:32:00   \n",
       "19800   83                              NaN              NaN         07:40:00   \n",
       "\n",
       "trip_id                    19-742--1-MF3-48 19-742--1-MF3-50 19-742--1-MF3-52  \\\n",
       "stop_id stop_sequence_real                                                      \n",
       "21270   1                          07:32:00         08:46:00         09:15:00   \n",
       "47093   2                          07:32:00         08:46:00         09:15:00   \n",
       "15715   3                          07:32:00         08:46:00         09:15:00   \n",
       "15716   4                          07:33:00         08:48:00         09:17:00   \n",
       "15717   5                          07:33:00         08:48:00         09:17:00   \n",
       "...                                     ...              ...              ...   \n",
       "47928   79                         09:04:00         10:05:00         10:34:00   \n",
       "11527   80                              NaN              NaN              NaN   \n",
       "1972    81                              NaN              NaN              NaN   \n",
       "1560    82                              NaN              NaN              NaN   \n",
       "19800   83                              NaN              NaN              NaN   \n",
       "\n",
       "trip_id                    19-742--1-MF3-54 19-742--1-MF3-56 19-742--1-MF3-58  \\\n",
       "stop_id stop_sequence_real                                                      \n",
       "21270   1                          11:48:00         12:48:00         13:18:00   \n",
       "47093   2                          11:48:00         12:48:00         13:18:00   \n",
       "15715   3                          11:48:00         12:48:00         13:18:00   \n",
       "15716   4                          11:49:00         12:49:00         13:19:00   \n",
       "15717   5                          11:49:00         12:49:00         13:19:00   \n",
       "...                                     ...              ...              ...   \n",
       "47928   79                         13:01:00         14:02:00         14:32:00   \n",
       "11527   80                              NaN              NaN              NaN   \n",
       "1972    81                              NaN              NaN              NaN   \n",
       "1560    82                              NaN              NaN              NaN   \n",
       "19800   83                              NaN              NaN              NaN   \n",
       "\n",
       "trip_id                    19-742--1-MF3-60  ... 29-742--1-Sun1-48  \\\n",
       "stop_id stop_sequence_real                   ...                     \n",
       "21270   1                          14:17:00  ...          13:24:00   \n",
       "47093   2                          14:17:00  ...          13:24:00   \n",
       "15715   3                          14:17:00  ...          13:24:00   \n",
       "15716   4                          14:18:00  ...          13:25:00   \n",
       "15717   5                          14:18:00  ...          13:25:00   \n",
       "...                                     ...  ...               ...   \n",
       "47928   79                         15:33:00  ...          14:37:00   \n",
       "11527   80                              NaN  ...               NaN   \n",
       "1972    81                              NaN  ...               NaN   \n",
       "1560    82                              NaN  ...               NaN   \n",
       "19800   83                              NaN  ...               NaN   \n",
       "\n",
       "trip_id                    29-742--1-Sun1-49 29-742--1-Sun2-1  \\\n",
       "stop_id stop_sequence_real                                      \n",
       "21270   1                           16:24:00         08:50:00   \n",
       "47093   2                           16:24:00         08:50:00   \n",
       "15715   3                           16:24:00         08:50:00   \n",
       "15716   4                           16:25:00         08:51:00   \n",
       "15717   5                           16:25:00         08:51:00   \n",
       "...                                      ...              ...   \n",
       "47928   79                          17:38:00         10:03:00   \n",
       "11527   80                               NaN              NaN   \n",
       "1972    81                               NaN              NaN   \n",
       "1560    82                               NaN              NaN   \n",
       "19800   83                               NaN              NaN   \n",
       "\n",
       "trip_id                    29-742--1-Sun2-47 29-742--1-Sun2-48  \\\n",
       "stop_id stop_sequence_real                                       \n",
       "21270   1                           11:25:00          13:24:00   \n",
       "47093   2                           11:25:00          13:24:00   \n",
       "15715   3                           11:25:00          13:24:00   \n",
       "15716   4                           11:26:00          13:25:00   \n",
       "15717   5                           11:26:00          13:25:00   \n",
       "...                                      ...               ...   \n",
       "47928   79                          12:40:00          14:37:00   \n",
       "11527   80                               NaN               NaN   \n",
       "1972    81                               NaN               NaN   \n",
       "1560    82                               NaN               NaN   \n",
       "19800   83                               NaN               NaN   \n",
       "\n",
       "trip_id                    29-742--1-Sun2-49 29-742--1-Sun3-12  \\\n",
       "stop_id stop_sequence_real                                       \n",
       "21270   1                           16:24:00          13:39:00   \n",
       "47093   2                           16:24:00          13:39:00   \n",
       "15715   3                           16:24:00          13:39:00   \n",
       "15716   4                           16:25:00          13:41:00   \n",
       "15717   5                           16:25:00          13:41:00   \n",
       "...                                      ...               ...   \n",
       "47928   79                          17:38:00          14:55:00   \n",
       "11527   80                               NaN               NaN   \n",
       "1972    81                               NaN               NaN   \n",
       "1560    82                               NaN               NaN   \n",
       "19800   83                               NaN               NaN   \n",
       "\n",
       "trip_id                    29-742--1-Sun3-15 29-742--1-Sun3-5 29-742--1-Sun4-5  \n",
       "stop_id stop_sequence_real                                                      \n",
       "21270   1                           14:16:00         09:00:00         12:24:00  \n",
       "47093   2                           14:16:00         09:00:00         12:24:00  \n",
       "15715   3                           14:16:00         09:00:00         12:24:00  \n",
       "15716   4                           14:18:00         09:01:00         12:25:00  \n",
       "15717   5                           14:18:00         09:01:00         12:25:00  \n",
       "...                                      ...              ...              ...  \n",
       "47928   79                          15:31:00         10:15:00         13:38:00  \n",
       "11527   80                               NaN              NaN              NaN  \n",
       "1972    81                               NaN              NaN              NaN  \n",
       "1560    82                               NaN              NaN              NaN  \n",
       "19800   83                               NaN              NaN              NaN  \n",
       "\n",
       "[83 rows x 246 columns]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1.pivot_table(index=['stop_id', 'stop_sequence_real'], columns='trip_id', values='arrival_time', aggfunc='first').sort_index(level=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
