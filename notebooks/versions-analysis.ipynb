{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyptvgtfs\n",
    "from pyptvgtfs import BRANCH_IDS, GTFS_FILE_FIELDS_TYPES, TABLE_NAMES, BRANCH_IDS_ALL\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSIONS = [\n",
    "    '20220403_025040',\n",
    "    '20230805_030129',\n",
    "    '20231021_105623',\n",
    "    '20240229_224711'\n",
    "]\n",
    "\n",
    "# VERSIONS x BRANCHES\n",
    "VERSIONS_BRANCHES = [(v, b) for v in VERSIONS for b in BRANCH_IDS]\n",
    "VERSIONS_BRANCHES_ALL = [(v, b) for v in VERSIONS for b in BRANCH_IDS_ALL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFS_LIST = [pyptvgtfs.process_gtfs_zip(f'../downloads/{f}/gtfs.zip', f) for f in VERSIONS]\n",
    "# Per file: 40s - 1m - 3m. 5 files: 2m - 5m. 7 files: 3m - 5sm. 2 files: 1m 30s - 2m\n",
    "\n",
    "DFK : dict[tuple, pd.DataFrame] = pd.concat(DFS_LIST, axis=0).set_index(['version_id', 'branch_id', 'table_name'])['df'].to_dict()\n",
    "\n",
    "DF : dict[str, dict[str, dict[str, pd.DataFrame]]] = {}\n",
    "for (vid, bid, table_name), df in DFK.items():\n",
    "    DF[vid] = DF.get(vid, {})\n",
    "    DF[vid][bid] = DF[vid].get(bid, {})\n",
    "    DF[vid][bid][table_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems that after at least August 2023 and since at least October 2023, the service_id for buses ('4') has changed from normal service names like all others (T2_1) to specific service names to each route (MF1-12-831-aus). For trip id, before, it's 2-831--2-T2-1, now, it's 12-831--1-MF1-102. Seems like the format of trip_ids in 4 (use '-' instead of '.') being different from other operation branches (1,2...) is still consistent among versions.\n",
    "# On one hand this change makes 'calendar' and 'calendar_dates' tables become longer, have more redundant data points, and the service_id becomes longer. On the other hand, more specific service patterns to each route can be more informative and useful for table joining or querying about the service time of one specific route.\n",
    "# The change might make the 'trips table becomes somewhat longer, but an analysis of trips len for opbranch=4 for each version shows that length of trips table was not significantly too different.\n",
    "\n",
    "# Proof:\n",
    "DFK['20230805_030129', '4', 'trips'].head()\n",
    "DFK['20231021_105623', '4', 'trips'].head()\n",
    "for version in VERSIONS:\n",
    "    print(version, len(DFK[version, '4', 'trips']))\n",
    "\n",
    "\n",
    "# Assert all shape_id contains route_id\n",
    "for vid in VERSIONS:\n",
    "    for bid in BRANCH_IDS:\n",
    "        assert DF[vid][bid]['trips'].dropna(subset=['route_id', 'shape_id']).apply(lambda x: x['route_id'] in x['shape_id'], axis=1).all(), (vid, bid)\n",
    "\n",
    "# Assert all shape start with distance = 0\n",
    "for vid in VERSIONS:\n",
    "    for bid in BRANCH_IDS:\n",
    "        assert DF[vid][bid]['shapes'][DF[vid][bid]['shapes']['shape_pt_sequence'] == 1]['shape_dist_traveled'].unique() == [0], (vid, bid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(monday, tuesday, wednesday, thursday, friday, saturday, sunday, start_date, end_date):\n",
    "    # Get list of dates based on week pattern and date range\n",
    "    week_pattern = [bool(int(monday)), bool(int(tuesday)), bool(int(wednesday)), bool(int(thursday)), bool(int(friday)), bool(int(saturday)), bool(int(sunday))]\n",
    "    start_date = pd.to_datetime(start_date, format='%Y%m%d')\n",
    "    end_date = pd.to_datetime(end_date, format='%Y%m%d')\n",
    "    dates = pd.date_range(start_date, end_date)\n",
    "    return dates[[week_pattern[i] for i in dates.dayofweek]]\n",
    "    \n",
    "def get_dates_df_calendar(df_calendar: pd.DataFrame, df_calendar_dates: pd.DataFrame):\n",
    "    \n",
    "    weekdate_columns = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    daterange_columns = ['start_date', 'end_date']\n",
    "    pattern_columns = weekdate_columns + daterange_columns\n",
    "    \n",
    "    # Drop duplicates to reduce the number of rows to be processed\n",
    "    df_dates = df_calendar[pattern_columns].drop_duplicates()\n",
    "    \n",
    "    # Get date list based on week pattern and date range\n",
    "    df_dates['date'] = df_dates.apply(lambda x: get_dates(x['monday'], x['tuesday'], x['wednesday'], x['thursday'], x['friday'], x['saturday'], x['sunday'], x['start_date'], x['end_date']), axis=1)\n",
    "\n",
    "    df_dates['date'] = df_dates['date'].apply(lambda x: [y.strftime('%Y%m%d') for y in x])\n",
    "    \n",
    "    # Join the date list with the original calendar table\n",
    "    df_dates = pd.merge(df_calendar, df_dates, on=pattern_columns, how='left')\n",
    "    \n",
    "    # Explode the date list into separate rows\n",
    "    df_dates = df_dates[['service_id', 'date']].explode('date')\n",
    "\n",
    "    # Join the date df with the calendar_dates df\n",
    "    df_dates = pd.merge(df_dates, df_calendar_dates.astype({'date': str, 'exception_type': str}), on=['service_id', 'date'], how='outer')\n",
    "    \n",
    "    # Drop 2 and keep 1 and NaN\n",
    "    df_dates = df_dates[df_dates['exception_type'] != '2'].reset_index(drop=True)\n",
    "\n",
    "    return df_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['service_ids'] = pd.DataFrame(DF[vid][bid]['calendar']['service_id'].drop_duplicates().reset_index(drop=True))\n",
    "    DF[vid][bid]['route_ids'] = pd.DataFrame(DF[vid][bid]['routes']['route_id'].drop_duplicates().reset_index(drop=True))\n",
    "    DF[vid][bid]['trip_ids'] = pd.DataFrame(DF[vid][bid]['trips']['trip_id'].drop_duplicates().reset_index(drop=True))\n",
    "    DF[vid][bid]['shape_ids'] = pd.DataFrame(DF[vid][bid]['shapes']['shape_id'].drop_duplicates().reset_index(drop=True))\n",
    "# 1s - 2s\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    # Get all types of delimiters\n",
    "    DF[vid][bid]['patterns'] = {}\n",
    "    DF[vid][bid]['patterns'] = {}\n",
    "    DF[vid][bid]['delimiters'] = {}\n",
    "    for id_name in ['service_id', 'route_id', 'trip_id', 'shape_id']:\n",
    "        id_pattern = DF[vid][bid][f'{id_name}s'][id_name].str.replace(r'[a-zA-Z0-9]+', '0', regex=True).drop_duplicates()\n",
    "        DF[vid][bid]['patterns'][id_name] = id_pattern.unique()\n",
    "        id_pattern = id_pattern.str.replace(r'[0]', '', regex=True).unique()\n",
    "        # Sum all in DF[vid][bid]['patterns'][id_name] and remove duplicates\n",
    "        DF[vid][bid]['delimiters'][id_name] = set(''.join(id_pattern))\n",
    "# 2s - 5s\n",
    "        \n",
    "ID_PATTERNS = {\n",
    "    k: pd.DataFrame(\n",
    "        data=[(vid, bid, DF[vid][bid][\"patterns\"][k]) for vid, bid in VERSIONS_BRANCHES],\n",
    "        columns=[\"version_id\", \"branch_id\", \"pattern\"],\n",
    "    )\n",
    "    .explode(\"pattern\")\n",
    "    .groupby(\"pattern\")[\"branch_id\"]\n",
    "    .apply(lambda x: sorted(int(i) for i in x.unique()))\n",
    "    .to_dict()\n",
    "    for k in [\"service_id\", \"route_id\", \"trip_id\", \"shape_id\"]\n",
    "}\n",
    "\n",
    "\n",
    "ID_PATTERNS == {\n",
    "    \"service_id\": {\n",
    "        \"0\": [1, 2, 3, 4, 5, 6, 10, 11],\n",
    "        \"0+0\": [1, 2, 3, 4, 5, 6, 10],\n",
    "        \"0+0_0\": [1, 2, 3, 4, 5, 6, 10],\n",
    "        \"0-0-0-0\": [4],\n",
    "        \"0-0-0-0-0\": [4],\n",
    "        \"0_0\": [1, 2, 3, 4, 5, 6, 10],\n",
    "    },\n",
    "    \"route_id\": {\n",
    "        \"0-0-0-0\": [1, 2, 3, 4, 5, 6, 10, 11],\n",
    "        \"0-0-0-0-0\": [1, 2, 3, 4, 5, 6, 10],\n",
    "    },\n",
    "    \"trip_id\": {\n",
    "        \"0-0--0-0-0\": [4],\n",
    "        \"0-0-0-0-0-0\": [4],\n",
    "        \"0.0.0-0-0-0-0.0.0\": [1, 2, 3, 5, 6, 10],\n",
    "        \"0.0.0-0-0-0.0.0\": [1, 2, 3, 5, 6, 10, 11],\n",
    "    },\n",
    "    \"shape_id\": {\n",
    "        \"0-0-0-0-0.0.0\": [1, 2, 3, 4, 5, 6, 10],\n",
    "        \"0-0-0-0.0.0\": [1, 2, 3, 4, 5, 6, 10, 11],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['service_ids']['service_class'] = DF[vid][bid]['service_ids']['service_id'].apply(lambda x: x.split('-')[0].split('+')[0].split('_')[0])\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    df_route_idx = DF[vid][bid]['route_ids']['route_id'].apply(lambda x: x.split('-'))\n",
    "    DF[vid][bid]['route_ids']['route_code'] = df_route_idx.apply(lambda x: x[1])\n",
    "    DF[vid][bid]['route_ids']['route_code_extra'] = df_route_idx.apply(lambda x: x[2] if len(x) >= 5 else '')\n",
    "    DF[vid][bid]['route_ids']['route_no'] = df_route_idx.apply(lambda x: x[-1])\n",
    "    DF[vid][bid]['route_ids']['branch'] = df_route_idx.apply(lambda x: x[0])\n",
    "    DF[vid][bid]['route_ids']['range'] = df_route_idx.apply(lambda x: x[-2])\n",
    "\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    df_shape_idx = DF[vid][bid]['shape_ids']['shape_id'].apply(lambda x: x.split('.'))\n",
    "    df_route_id = df_shape_idx.apply(lambda x: x[0])\n",
    "    df_route_idx = df_route_id.apply(lambda x: x.split('-'))\n",
    "\n",
    "    DF[vid][bid]['shape_ids']['route_id'] = df_route_id\n",
    "    DF[vid][bid]['shape_ids']['route_code'] = df_route_idx.apply(lambda x: x[1])\n",
    "    DF[vid][bid]['shape_ids']['route_code_extra'] = df_route_idx.apply(lambda x: x[2] if len(x) >= 5 else '')\n",
    "    DF[vid][bid]['shape_ids']['route_no'] = df_route_idx.apply(lambda x: x[-1])\n",
    "    DF[vid][bid]['shape_ids']['branch'] = df_route_idx.apply(lambda x: x[0])\n",
    "    DF[vid][bid]['shape_ids']['direction'] = df_shape_idx.apply(lambda x: x[2])\n",
    "    DF[vid][bid]['shape_ids']['range'] = df_route_idx.apply(lambda x: x[-2])\n",
    "    DF[vid][bid]['shape_ids']['shape_no'] = df_shape_idx.apply(lambda x: x[1])\n",
    "# 1s\n",
    "    \n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    if bid == '4':\n",
    "        df_trip_idx = DF[vid][bid]['trip_ids']['trip_id'].apply(lambda x: x.split('-'))\n",
    "        DF[vid][bid]['trip_ids']['route_code'] = df_trip_idx.apply(lambda x: x[1])\n",
    "        DF[vid][bid]['trip_ids']['route_code_extra'] = df_trip_idx.apply(lambda x: x[2])\n",
    "        DF[vid][bid]['trip_ids']['route_no'] = df_trip_idx.apply(lambda x: x[3])\n",
    "        DF[vid][bid]['trip_ids']['branch'] = df_trip_idx.apply(lambda x: x[0])\n",
    "        DF[vid][bid]['trip_ids']['service_class'] = df_trip_idx.apply(lambda x: x[4])\n",
    "        DF[vid][bid]['trip_ids']['trip_no'] = df_trip_idx.apply(lambda x: x[5])\n",
    "    else:\n",
    "        df_trip_idx = DF[vid][bid]['trip_ids']['trip_id'].apply(lambda x: x.split('.'))\n",
    "        df_route_id = df_trip_idx.apply(lambda x: x[2])\n",
    "        df_route_idx = df_route_id.apply(lambda x: x.split('-'))\n",
    "\n",
    "        # DF[vid][bid]['trip_ids']['route_id'] = df_route_id\n",
    "        # DF[vid][bid]['trip_ids']['direction'] = df_trip_idx.apply(lambda x: x[4])\n",
    "        # DF[vid][bid]['trip_ids']['shape_no'] = df_trip_idx.apply(lambda x: x[3])\n",
    "        # DF[vid][bid]['trip_ids']['range'] = df_route_idx.apply(lambda x: x[-2])\n",
    "        \n",
    "        DF[vid][bid]['trip_ids']['route_code'] = df_route_idx.apply(lambda x: x[1])\n",
    "        DF[vid][bid]['trip_ids']['route_code_extra'] = df_route_idx.apply(lambda x: x[2] if len(x) >= 5 else '')\n",
    "        DF[vid][bid]['trip_ids']['route_no'] = df_route_idx.apply(lambda x: x[-1])\n",
    "        DF[vid][bid]['trip_ids']['branch'] = df_route_idx.apply(lambda x: x[0])\n",
    "        DF[vid][bid]['trip_ids']['service_class'] = df_trip_idx.apply(lambda x: x[1])\n",
    "        DF[vid][bid]['trip_ids']['trip_no'] = df_trip_idx.apply(lambda x: x[0])\n",
    "# 7s - 20s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['dates'] = get_dates_df_calendar(DF[vid][bid]['calendar'], DF[vid][bid]['calendar_dates'])\n",
    "# 1s - 5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['route_services'] = DF[vid][bid]['trips'][['route_id', 'service_id']].drop_duplicates().reset_index(drop=True)\n",
    "    DF[vid][bid]['route_services'] = pd.merge(DF[vid][bid]['route_services'], DF[vid][bid]['route_ids'], on='route_id', how='left')\n",
    "    DF[vid][bid]['route_services'] = pd.merge(DF[vid][bid]['route_services'], DF[vid][bid]['service_ids'], on='service_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['route_service_dates'] = pd.merge(DF[vid][bid]['route_services'], DF[vid][bid]['dates'], on='service_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['route_date_services'] = DF[vid][bid]['route_service_dates'].groupby(['route_id', 'date']).aggregate({'service_id': 'unique', 'service_class': 'unique'}).reset_index()\n",
    "    # 7s - 20s\n",
    "\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    # Assert that for each route_id, for each date, there is only one service_class\n",
    "    assert DF[vid][bid]['route_date_services']['service_class'].apply(lambda x: len(x) == 1).all(), (vid, bid)\n",
    "    # Some route_id - date has more than 1 service_id. These occurs in 1 2 5 6, and 4 prior to 4's service_id format change after August 2023. Other than that, all route_id - date has only 1 service_id.\n",
    "    ans = DF[vid][bid]['route_date_services']['service_id'].apply(lambda x: len(x) == 1).all()\n",
    "    print(vid, bid, (ans if ans else ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220403_025040 4\n",
      "20231021_105623 4\n",
      "20240229_224711 4\n"
     ]
    }
   ],
   "source": [
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['route_date_services_2'] = DF[vid][bid]['route_service_dates'].groupby(['route_code', 'route_code_extra', 'date']).aggregate({'service_id': 'unique', 'service_class': 'unique'}).reset_index()\n",
    "    # 7s - 20s\n",
    "\n",
    "\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    # Some route_code - date has more than 1 service_class. Most of these only occur in 4.\n",
    "    if not DF[vid][bid]['route_date_services_2']['service_class'].apply(lambda x: len(x) == 1).all():\n",
    "        print(vid, bid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_code</th>\n",
       "      <th>route_code_extra</th>\n",
       "      <th>date</th>\n",
       "      <th>service_id</th>\n",
       "      <th>service_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>200</td>\n",
       "      <td></td>\n",
       "      <td>20240223</td>\n",
       "      <td>[MF10-14-200-aus, MF5-17-200-aus, MF15-33-200-...</td>\n",
       "      <td>[MF10, MF5, MF15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>200</td>\n",
       "      <td></td>\n",
       "      <td>20240224</td>\n",
       "      <td>[Sat4-14-200-aus, Sat4-17-200-aus, Sat5-33-200...</td>\n",
       "      <td>[Sat4, Sat5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>200</td>\n",
       "      <td></td>\n",
       "      <td>20240226</td>\n",
       "      <td>[MF6-14-200-aus, MF1-17-200-aus, MF11-33-200-aus]</td>\n",
       "      <td>[MF6, MF1, MF11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>200</td>\n",
       "      <td></td>\n",
       "      <td>20240227</td>\n",
       "      <td>[MF7-14-200-aus, MF2-17-200-aus, MF12-33-200-aus]</td>\n",
       "      <td>[MF7, MF2, MF12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>200</td>\n",
       "      <td></td>\n",
       "      <td>20240228</td>\n",
       "      <td>[MF8-14-200-aus, MF3-17-200-aus, MF13-33-200-aus]</td>\n",
       "      <td>[MF8, MF3, MF13]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    route_code route_code_extra      date  \\\n",
       "525        200                   20240223   \n",
       "526        200                   20240224   \n",
       "528        200                   20240226   \n",
       "529        200                   20240227   \n",
       "530        200                   20240228   \n",
       "\n",
       "                                            service_id      service_class  \n",
       "525  [MF10-14-200-aus, MF5-17-200-aus, MF15-33-200-...  [MF10, MF5, MF15]  \n",
       "526  [Sat4-14-200-aus, Sat4-17-200-aus, Sat5-33-200...       [Sat4, Sat5]  \n",
       "528  [MF6-14-200-aus, MF1-17-200-aus, MF11-33-200-aus]   [MF6, MF1, MF11]  \n",
       "529  [MF7-14-200-aus, MF2-17-200-aus, MF12-33-200-aus]   [MF7, MF2, MF12]  \n",
       "530  [MF8-14-200-aus, MF3-17-200-aus, MF13-33-200-aus]   [MF8, MF3, MF13]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF['20240229_224711']['4']['route_date_services_2'][DF[vid]['4']['route_date_services_2']['service_class'].apply(lambda x: len(x) > 1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    DF[vid][bid]['route_date_services_3'] = DF[vid][bid]['route_service_dates'].groupby(['route_code', 'route_code_extra', 'branch', 'date']).aggregate({'service_id': 'unique', 'service_class': 'unique'}).reset_index()\n",
    "    # 7s - 20s\n",
    "\n",
    "# Proof that route_code - branch - date has only 1 service_class.\n",
    "for vid, bid in VERSIONS_BRANCHES:\n",
    "    # Assert route_code - branch - date has only 1 service_class.\n",
    "    assert DF[vid][bid]['route_date_services_3']['service_class'].apply(lambda x: len(x) == 1).all(), (vid, bid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate all trips_full\n",
    "DFTRIPSFULL : pd.DataFrame = pd.concat([DF[bid]['trips_full'] for bid in BRANCH_IDS])\n",
    "\n",
    "# Confirm that each shape_id has only one stop_pattern (stop sequence)\n",
    "assert DFTRIPSFULL.groupby('shape_id')['stop_pattern'].nunique().unique() == [1]\n",
    "\n",
    "\n",
    "# Get full information about trips, including stop sequence and route, shape ids\n",
    "for bid in BRANCH_IDS:\n",
    "    DF[bid]['trips_full'] = pd.merge(DF[bid]['trips'], DF[bid]['trip_stops'], on='trip_id')\n",
    "    DF[bid]['trips_full']['stop_pattern'] = DF[bid]['trips_full']['stop_id'].apply(lambda x: '-'.join([str(i) for i in x]))\n",
    "# 3s - 5s\n",
    "\n",
    "for bid in BRANCH_IDS:\n",
    "    # Assert that all shape_pt_sequence are continuous\n",
    "    assert DF[bid]['shapes_pt_sequence']['shape_pt_sequence'].apply(lambda x: x[-1] - x[0] + 1 == len(x)).all(), bid\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
