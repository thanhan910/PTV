{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import time\n",
    "import subprocess\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import requests\n",
    "import shutil\n",
    "import os\n",
    "import io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(dbname=\"postgres\", user=\"postgres\", password=\"postgres\", host=\"localhost\", port=\"5432\")\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"DROP DATABASE IF EXISTS gtfs;\")\n",
    "cursor.execute(\"CREATE DATABASE gtfs;\")\n",
    "cursor.close()\n",
    "\n",
    "conn = psycopg2.connect(dbname=\"gtfs\", user=\"postgres\", password=\"postgres\", host=\"localhost\", port=\"5432\")\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://data.ptv.vic.gov.au/downloads/gtfs.zip\n",
    "# Download gtfs.zip from the above link and extract it to the folder where this script is located\n",
    "\n",
    "DB_TABLES = {}\n",
    "def gtfs_obj(gtfs_zip: zipfile.ZipFile) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function reads a GTFS .zip file and returns a dictionary of pandas DataFrames\n",
    "    \"\"\"\n",
    "    DFK = {}\n",
    "    for item in gtfs_zip.namelist():\n",
    "        if not item.endswith('/'): # Check if the item is a directory\n",
    "            continue\n",
    "        mode_id = item.strip('/')\n",
    "        \n",
    "        DFK[mode_id] = {}\n",
    "\n",
    "        google_transit_zip_path = f\"{mode_id}/google_transit.zip\"\n",
    "        with gtfs_zip.open(google_transit_zip_path) as google_transit_file:\n",
    "            with zipfile.ZipFile(google_transit_file, 'r') as transit_zip:\n",
    "\n",
    "                for nested_file_name in transit_zip.namelist():\n",
    "                \n",
    "                    if not nested_file_name.endswith('.txt'):\n",
    "                        continue\n",
    "            \n",
    "                    table_name = nested_file_name.removesuffix('.txt')\n",
    "            \n",
    "                    with transit_zip.open(nested_file_name) as nested_file:\n",
    "                        # Save the contents of the file to a file\n",
    "                        csv_file_path = os.path.abspath(f\"{table_name}_{mode_id}.csv\")\n",
    "                        DB_TABLES[(table_name, mode_id)] = csv_file_path\n",
    "                        with open(csv_file_path, \"wb\") as f:\n",
    "                            f.write(nested_file.read())\n",
    "                        \n",
    "                        # df = pd.read_csv(nested_file, sep=\",\", low_memory=False, encoding='utf-8', dtype=str)\n",
    "                        # DFK[mode_id][table_name] = df\n",
    "    \n",
    "    return DFK\n",
    "\n",
    "\n",
    "def read_gtfs(url_or_path : str = \"http://data.ptv.vic.gov.au/downloads/gtfs.zip\") -> dict[str, dict[str, pd.DataFrame]]:  \n",
    "\n",
    "    assert url_or_path.endswith('.zip'), \"File must be a .zip file\"   \n",
    "    \n",
    "    if \":\" in url_or_path: # If url_or_path is a URL\n",
    "        \n",
    "        response = requests.get(url_or_path, stream=True)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Create a ZipFile object from the response content\n",
    "            with zipfile.ZipFile(io.BytesIO(response.content)) as gtfs_zip:\n",
    "                return gtfs_obj(gtfs_zip=gtfs_zip)\n",
    "    \n",
    "    with zipfile.ZipFile(url_or_path, 'r') as gtfs_zip:\n",
    "        return gtfs_obj(gtfs_zip=gtfs_zip)\n",
    "    \n",
    "dfk = read_gtfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GTFS_FILE_FIELDS = {\n",
    "    'agency': ['agency_id', 'agency_name', 'agency_url', 'agency_timezone', 'agency_lang'],\n",
    "    'calendar': ['service_id', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'start_date', 'end_date'],\n",
    "    'calendar_dates': ['service_id', 'date', 'exception_type'],\n",
    "    'routes': ['route_id', 'agency_id', 'route_short_name', 'route_long_name', 'route_type', 'route_color', 'route_text_color'],\n",
    "    'trips': ['route_id', 'service_id', 'trip_id', 'shape_id', 'trip_headsign', 'direction_id'],\n",
    "    'stops': ['stop_id', 'stop_name', 'stop_lat', 'stop_lon'],\n",
    "    'stop_times': ['trip_id', 'arrival_time', 'departure_time', 'stop_id', 'stop_sequence', 'stop_headsign', 'pickup_type', 'drop_off_type', 'shape_dist_traveled'],\n",
    "    'shapes': ['shape_id', 'shape_pt_lat', 'shape_pt_lon', 'shape_pt_sequence', 'shape_dist_traveled']\n",
    "}\n",
    "\n",
    "GTFS_FILE_FIELDS_TYPES = {\n",
    "    'agency': {'agency_id': str, 'agency_name': str, 'agency_url': str, 'agency_timezone': str, 'agency_lang': str},\n",
    "    'calendar': {'service_id': str, 'monday': int, 'tuesday': int, 'wednesday': int, 'thursday': int, 'friday': int, 'saturday': int, 'sunday': int, 'start_date': str, 'end_date': str},\n",
    "    'calendar_dates': {'service_id': str, 'date': str, 'exception_type': str},\n",
    "    'routes': {'route_id': str, 'agency_id': str, 'route_short_name': str, 'route_long_name': str, 'route_type': str, 'route_color': str, 'route_text_color' : str},\n",
    "    'trips': {'route_id': str, 'service_id': str, 'trip_id': str, 'shape_id': str, 'trip_headsign': str, 'direction_id' : str},\n",
    "    'stops': {'stop_id': str, 'stop_name': str, 'stop_lat': np.float64, 'stop_lon': np.float64},\n",
    "    'stop_times': {'trip_id': str, 'arrival_time': str, 'departure_time': str, 'stop_id': str, 'stop_sequence': int, 'stop_headsign': str, 'pickup_type': str, 'drop_off_type': str, 'shape_dist_traveled': np.float64},\n",
    "    'shapes': {'shape_id': str, 'shape_pt_lat': np.float64, 'shape_pt_lon': np.float64, 'shape_pt_sequence': int, 'shape_dist_traveled': np.float64},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table agency_1\n",
      "Table agency created\n",
      "Creating table calendar_1\n",
      "Table calendar created\n",
      "Creating table calendar_dates_1\n",
      "Table calendar_dates created\n",
      "Creating table routes_1\n",
      "Table routes created\n",
      "Creating table shapes_1\n",
      "Table shapes created\n",
      "Creating table stops_1\n",
      "Table stops created\n",
      "Creating table stop_times_1\n",
      "Table stop_times created\n",
      "Creating table trips_1\n",
      "Table trips created\n",
      "Creating table agency_10\n",
      "Table agency created\n",
      "Creating table calendar_10\n",
      "Table calendar created\n",
      "Creating table calendar_dates_10\n",
      "Table calendar_dates created\n",
      "Creating table routes_10\n",
      "Table routes created\n",
      "Creating table shapes_10\n",
      "Table shapes created\n",
      "Creating table stops_10\n",
      "Table stops created\n",
      "Creating table stop_times_10\n",
      "Table stop_times created\n",
      "Creating table trips_10\n",
      "Table trips created\n",
      "Creating table agency_11\n",
      "Table agency created\n",
      "Creating table calendar_11\n",
      "Table calendar created\n",
      "Creating table calendar_dates_11\n",
      "Table calendar_dates created\n",
      "Creating table routes_11\n",
      "Table routes created\n",
      "Creating table shapes_11\n",
      "Table shapes created\n",
      "Creating table stops_11\n",
      "Table stops created\n",
      "Creating table stop_times_11\n",
      "Table stop_times created\n",
      "Creating table trips_11\n",
      "Table trips created\n",
      "Creating table agency_2\n",
      "Table agency created\n",
      "Creating table calendar_2\n",
      "Table calendar created\n",
      "Creating table calendar_dates_2\n",
      "Table calendar_dates created\n",
      "Creating table routes_2\n",
      "Table routes created\n",
      "Creating table shapes_2\n",
      "Table shapes created\n",
      "Creating table stops_2\n",
      "Table stops created\n",
      "Creating table stop_times_2\n",
      "Table stop_times created\n",
      "Creating table trips_2\n",
      "Table trips created\n",
      "Creating table agency_3\n",
      "Table agency created\n",
      "Creating table calendar_3\n",
      "Table calendar created\n",
      "Creating table calendar_dates_3\n",
      "Table calendar_dates created\n",
      "Creating table routes_3\n",
      "Table routes created\n",
      "Creating table shapes_3\n",
      "Table shapes created\n",
      "Creating table stops_3\n",
      "Table stops created\n",
      "Creating table stop_times_3\n",
      "Table stop_times created\n",
      "Creating table trips_3\n",
      "Table trips created\n",
      "Creating table agency_4\n",
      "Table agency created\n",
      "Creating table calendar_4\n",
      "Table calendar created\n",
      "Creating table calendar_dates_4\n",
      "Table calendar_dates created\n",
      "Creating table routes_4\n",
      "Table routes created\n",
      "Creating table shapes_4\n",
      "Table shapes created\n",
      "Creating table stops_4\n",
      "Table stops created\n",
      "Creating table stop_times_4\n",
      "Table stop_times created\n",
      "Creating table trips_4\n",
      "Table trips created\n",
      "Creating table agency_5\n",
      "Table agency created\n",
      "Creating table calendar_5\n",
      "Table calendar created\n",
      "Creating table calendar_dates_5\n",
      "Table calendar_dates created\n",
      "Creating table routes_5\n",
      "Table routes created\n",
      "Creating table shapes_5\n",
      "Table shapes created\n",
      "Creating table stops_5\n",
      "Table stops created\n",
      "Creating table stop_times_5\n",
      "Table stop_times created\n",
      "Creating table trips_5\n",
      "Table trips created\n",
      "Creating table agency_6\n",
      "Table agency created\n",
      "Creating table calendar_6\n",
      "Table calendar created\n",
      "Creating table calendar_dates_6\n",
      "Table calendar_dates created\n",
      "Creating table routes_6\n",
      "Table routes created\n",
      "Creating table shapes_6\n",
      "Table shapes created\n",
      "Creating table stops_6\n",
      "Table stops created\n",
      "Creating table stop_times_6\n",
      "Table stop_times created\n",
      "Creating table trips_6\n",
      "Table trips created\n",
      "Creating table agency_7\n",
      "Table agency created\n",
      "Creating table routes_7\n",
      "Table routes created\n",
      "Creating table trips_7\n",
      "Table trips created\n",
      "Creating table stops_7\n",
      "Table stops created\n",
      "Creating table calendar_7\n",
      "Table calendar created\n",
      "Creating table calendar_dates_7\n",
      "Table calendar_dates created\n",
      "Creating table shapes_7\n",
      "Table shapes created\n",
      "Creating table stop_times_7\n",
      "Table stop_times created\n",
      "Creating table agency_8\n",
      "Table agency created\n",
      "Creating table routes_8\n",
      "Table routes created\n",
      "Creating table trips_8\n",
      "Table trips created\n",
      "Creating table stops_8\n",
      "Table stops created\n",
      "Creating table calendar_8\n",
      "Table calendar created\n",
      "Creating table calendar_dates_8\n",
      "Table calendar_dates created\n",
      "Creating table shapes_8\n",
      "Table shapes created\n",
      "Creating table stop_times_8\n",
      "Table stop_times created\n"
     ]
    }
   ],
   "source": [
    "BAT_COMMANDS = []\n",
    "\n",
    "BAT_COMMANDS.append(f\"set PGPASSWORD=postgres\")\n",
    "\n",
    "def determine_type(type_name):\n",
    "    if type_name == \"str\":\n",
    "        return \"TEXT\"\n",
    "    elif type_name == \"int\" or type_name == \"np.int64\" or type_name == \"np.int32\" or type_name == \"np.int16\" or type_name == \"np.int8\":\n",
    "        return \"INTEGER\"\n",
    "    elif type_name == \"float\" or type_name == \"np.float64\" or type_name == \"np.float32\" or type_name == \"np.float16\" or type_name == \"np.float8\":\n",
    "        return \"REAL\"\n",
    "    else:\n",
    "        return \"TEXT\"\n",
    "# conn.close()\n",
    "# conn = psycopg2.connect(dbname=\"gtfs\", user=\"postgres\", password=\"postgres\", host=\"localhost\", port=\"5432\")\n",
    "# cursor = conn.cursor()\n",
    "for (table_name, mode_id), csv_file_path in DB_TABLES.items():\n",
    "    db_table_name = f\"{table_name}_{mode_id}\"\n",
    "    print(f\"Creating table {db_table_name}\")\n",
    "    data_types = GTFS_FILE_FIELDS_TYPES[table_name]\n",
    "    # df = pd.read_csv(csv_file_path, sep=\",\", low_memory=False, encoding='utf-8', dtype=GTFS_FILE_FIELDS_TYPES[db_table_name])\n",
    "    # df.to_sql(db_table_name, conn, if_exists='replace', index=False)\n",
    "    # os.remove(csv_file_path)\n",
    "    # print(f\"Table {table_name} created\")\n",
    "    # Import csv to postgres\n",
    "    cursor.execute(f\"DROP TABLE IF EXISTS {db_table_name}\")\n",
    "    cursor.execute(f\"CREATE TABLE {db_table_name} ({', '.join([f'{k} {determine_type(v.__name__)}' for k, v in data_types.items()])})\")\n",
    "    # os.remove(csv_file_path)\n",
    "    BAT_COMMANDS.append(f\"psql -U postgres -d gtfs -c \\\"\\\\copy {db_table_name} FROM '{csv_file_path}' DELIMITER ',' CSV HEADER\\\"\")\n",
    "    # Remove the csv file\n",
    "    BAT_COMMANDS.append(f\"del {csv_file_path}\")\n",
    "    print(f\"Table {table_name} created\")\n",
    "with open(\"import_gtfs.bat\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(BAT_COMMANDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table agency_1\n",
      "Table agency deleted\n",
      "Creating table calendar_1\n",
      "Table calendar deleted\n",
      "Creating table calendar_dates_1\n",
      "Table calendar_dates deleted\n",
      "Creating table routes_1\n",
      "Table routes deleted\n",
      "Creating table shapes_1\n",
      "Table shapes deleted\n",
      "Creating table stops_1\n",
      "Table stops deleted\n",
      "Creating table stop_times_1\n",
      "Table stop_times deleted\n",
      "Creating table trips_1\n",
      "Table trips deleted\n",
      "Creating table agency_10\n",
      "Table agency deleted\n",
      "Creating table calendar_10\n",
      "Table calendar deleted\n",
      "Creating table calendar_dates_10\n",
      "Table calendar_dates deleted\n",
      "Creating table routes_10\n",
      "Table routes deleted\n",
      "Creating table shapes_10\n",
      "Table shapes deleted\n",
      "Creating table stops_10\n",
      "Table stops deleted\n",
      "Creating table stop_times_10\n",
      "Table stop_times deleted\n",
      "Creating table trips_10\n",
      "Table trips deleted\n",
      "Creating table agency_11\n",
      "Table agency deleted\n",
      "Creating table calendar_11\n",
      "Table calendar deleted\n",
      "Creating table calendar_dates_11\n",
      "Table calendar_dates deleted\n",
      "Creating table routes_11\n",
      "Table routes deleted\n",
      "Creating table shapes_11\n",
      "Table shapes deleted\n",
      "Creating table stops_11\n",
      "Table stops deleted\n",
      "Creating table stop_times_11\n",
      "Table stop_times deleted\n",
      "Creating table trips_11\n",
      "Table trips deleted\n",
      "Creating table agency_2\n",
      "Table agency deleted\n",
      "Creating table calendar_2\n",
      "Table calendar deleted\n",
      "Creating table calendar_dates_2\n",
      "Table calendar_dates deleted\n",
      "Creating table routes_2\n",
      "Table routes deleted\n",
      "Creating table shapes_2\n",
      "Table shapes deleted\n",
      "Creating table stops_2\n",
      "Table stops deleted\n",
      "Creating table stop_times_2\n",
      "Table stop_times deleted\n",
      "Creating table trips_2\n",
      "Table trips deleted\n",
      "Creating table agency_3\n",
      "Table agency deleted\n",
      "Creating table calendar_3\n",
      "Table calendar deleted\n",
      "Creating table calendar_dates_3\n",
      "Table calendar_dates deleted\n",
      "Creating table routes_3\n",
      "Table routes deleted\n",
      "Creating table shapes_3\n",
      "Table shapes deleted\n",
      "Creating table stops_3\n",
      "Table stops deleted\n",
      "Creating table stop_times_3\n",
      "Table stop_times deleted\n",
      "Creating table trips_3\n",
      "Table trips deleted\n",
      "Creating table agency_4\n",
      "Table agency deleted\n",
      "Creating table calendar_4\n",
      "Table calendar deleted\n",
      "Creating table calendar_dates_4\n",
      "Table calendar_dates deleted\n",
      "Creating table routes_4\n",
      "Table routes deleted\n",
      "Creating table shapes_4\n",
      "Table shapes deleted\n",
      "Creating table stops_4\n",
      "Table stops deleted\n",
      "Creating table stop_times_4\n",
      "Table stop_times deleted\n",
      "Creating table trips_4\n",
      "Table trips deleted\n",
      "Creating table agency_5\n",
      "Table agency deleted\n",
      "Creating table calendar_5\n",
      "Table calendar deleted\n",
      "Creating table calendar_dates_5\n",
      "Table calendar_dates deleted\n",
      "Creating table routes_5\n",
      "Table routes deleted\n",
      "Creating table shapes_5\n",
      "Table shapes deleted\n",
      "Creating table stops_5\n",
      "Table stops deleted\n",
      "Creating table stop_times_5\n",
      "Table stop_times deleted\n",
      "Creating table trips_5\n",
      "Table trips deleted\n",
      "Creating table agency_6\n",
      "Table agency deleted\n",
      "Creating table calendar_6\n",
      "Table calendar deleted\n",
      "Creating table calendar_dates_6\n",
      "Table calendar_dates deleted\n",
      "Creating table routes_6\n",
      "Table routes deleted\n",
      "Creating table shapes_6\n",
      "Table shapes deleted\n",
      "Creating table stops_6\n",
      "Table stops deleted\n",
      "Creating table stop_times_6\n",
      "Table stop_times deleted\n",
      "Creating table trips_6\n",
      "Table trips deleted\n",
      "Creating table agency_7\n",
      "Table agency deleted\n",
      "Creating table routes_7\n",
      "Table routes deleted\n",
      "Creating table trips_7\n",
      "Table trips deleted\n",
      "Creating table stops_7\n",
      "Table stops deleted\n",
      "Creating table calendar_7\n",
      "Table calendar deleted\n",
      "Creating table calendar_dates_7\n",
      "Table calendar_dates deleted\n",
      "Creating table shapes_7\n",
      "Table shapes deleted\n",
      "Creating table stop_times_7\n",
      "Table stop_times deleted\n",
      "Creating table agency_8\n",
      "Table agency deleted\n",
      "Creating table routes_8\n",
      "Table routes deleted\n",
      "Creating table trips_8\n",
      "Table trips deleted\n",
      "Creating table stops_8\n",
      "Table stops deleted\n",
      "Creating table calendar_8\n",
      "Table calendar deleted\n",
      "Creating table calendar_dates_8\n",
      "Table calendar_dates deleted\n",
      "Creating table shapes_8\n",
      "Table shapes deleted\n",
      "Creating table stop_times_8\n",
      "Table stop_times deleted\n"
     ]
    }
   ],
   "source": [
    "cursor.close()\n",
    "conn.close()\n",
    "conn = psycopg2.connect(dbname=\"gnaf\", user=\"postgres\", password=\"postgres\", host=\"localhost\", port=\"5432\")\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "for (table_name, mode_id), csv_file_path in DB_TABLES.items():\n",
    "    db_table_name = f\"{table_name}_{mode_id}\"\n",
    "    print(f\"Creating table {db_table_name}\")\n",
    "    data_types = GTFS_FILE_FIELDS_TYPES[table_name]\n",
    "    cursor.execute(f\"DROP TABLE IF EXISTS {db_table_name}\")\n",
    "    print(f\"Table {table_name} deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_NAMES = ['agency', 'calendar', 'calendar_dates', 'routes', 'shapes', 'stop_times', 'stops', 'trips']\n",
    "MODE_IDS = ['1', '2', '3', '4', '5', '6', '7', '8', '10', '11']\n",
    "DB_TABLES = {\n",
    "    (table, mode_id): f\"{mode_id}_{table}.csv\" for mode_id in MODE_IDS for table in TABLE_NAMES\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframes to the database\n",
    "\n",
    "# Create database, then connect to it\n",
    "\n",
    "conn = psycopg2.connect(dbname=\"postgres\", user=\"postgres\", password=\"postgres\", host=\"localhost\", port=\"5432\")\n",
    "cursor = conn.cursor()\n",
    "conn.autocommit = True\n",
    "# cursor.execute(\"CREATE DATABASE ptv_gtfs;\")\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(dbname=\"ptv_gtfs\", user=\"postgres\", password=\"postgres\", host=\"localhost\", port=\"5432\")\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "DB_TABLES = []\n",
    "for mode_id in MODE_IDS:\n",
    "    for table_name in TABLE_NAMES:\n",
    "        df = dfk[mode_id][table_name]\n",
    "        db_table_name = f\"{mode_id}_{table_name}\"\n",
    "        df.to_csv(f\"{db_table_name}.csv\", index=False)\n",
    "        db_tablepath = os.path.abspath(f\"{db_table_name}.csv\")\n",
    "        DB_TABLES.append((db_tablepath, db_table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnaf_dirpath = '../local/g-naf_feb24_allstates_gda2020_psv_1014/G-NAF/G-NAF FEBRUARY 2024'\n",
    "TABLES = []\n",
    "for dirpath, dirnames, filenames in os.walk(gnaf_dirpath):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.psv'):\n",
    "            assert filename.endswith('_psv.psv')\n",
    "            filepath = os.path.abspath(os.path.join(dirpath, filename))\n",
    "            # For each file, create a table in the database\n",
    "            tablename = filename.removesuffix('_psv.psv')\n",
    "            tablename = tablename.replace('-', '_')\n",
    "            tablename = tablename.replace(' ', '_')\n",
    "            # tablename = tablename.replace('_', '')\n",
    "            tablename = tablename.lower()\n",
    "            assert len(tablename) < 64\n",
    "            TABLES.append((tablename, filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set password for the postgres user\n",
    "# PGPASSWORD=postgres\n",
    "BAT_COMMANDS = []\n",
    "# subprocess.run('set PGPASSWORD=postgres', shell=True, check=True)\n",
    "BAT_COMMANDS.append('@echo on')\n",
    "BAT_COMMANDS.append('echo \"Setting password for the postgres user\"')\n",
    "BAT_COMMANDS.append('set PGPASSWORD=postgres')\n",
    "for table_name, psv_file_path in TABLES:\n",
    "    with open(psv_file_path, 'r') as f:\n",
    "        columns = f.readline().strip()\n",
    "    columns = columns.split('|')\n",
    "    # columns = [col.lower() for col in columns]\n",
    "    # Drop the table if it already exists\n",
    "    drop_table_query = f\"DROP TABLE IF EXISTS {table_name};\"\n",
    "    cursor.execute(drop_table_query)\n",
    "    # Create the table\n",
    "    create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(f\"{col} TEXT\" for col in columns)});\"\n",
    "    cursor.execute(create_table_query)\n",
    "    print(f\"Table {table_name} created\")\n",
    "\n",
    "    # Use PostgreSQL's COPY command to import data from the PSV file into the table\n",
    "    BAT_COMMANDS.append(f'echo \"Creating table {table_name}\"')\n",
    "    bat_command = f\"psql -d gnaf -U postgres -c \\\"\\\\copy {table_name} FROM '{psv_file_path}' DELIMITER '|' CSV HEADER\\\"\"\n",
    "    BAT_COMMANDS.append(bat_command)\n",
    "    BAT_COMMANDS.append(f'echo \"Table {table_name} created and populated with data from {psv_file_path}\"')\n",
    "    # subprocess.run(['psql', '-d', 'gnaf', '-U', 'postgres', '-c', f\"COPY {table_name} FROM '{psv_file_path}' DELIMITER '|' CSV HEADER\"], check=True)\n",
    "    # print(f\"Table {table_name} created and populated with data from {psv_file_path}\")\n",
    "with open('local-import_data.bat', 'w') as f:\n",
    "    f.write('\\n'.join(BAT_COMMANDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''SELECT datname AS \"Database\", \n",
    "       pg_size_pretty(pg_database_size(datname)) AS \"Size\" \n",
    "FROM pg_database;'''\n",
    "cursor.execute(sql)\n",
    "result = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set password for the postgres user\n",
    "# PGPASSWORD=postgres\n",
    "BAT_COMMANDS = []\n",
    "# subprocess.run('set PGPASSWORD=postgres', shell=True, check=True)\n",
    "BAT_COMMANDS.append('@echo on')\n",
    "BAT_COMMANDS.append('echo \"Setting password for the postgres user\"')\n",
    "BAT_COMMANDS.append('set PGPASSWORD=postgres')\n",
    "for table_name, psv_file_path in TABLES:\n",
    "    with open(psv_file_path, 'r') as f:\n",
    "        columns = f.readline().strip()\n",
    "    columns = columns.split('|')\n",
    "    # columns = [col.lower() for col in columns]\n",
    "    # Drop the table if it already exists\n",
    "    drop_table_query = f\"DROP TABLE IF EXISTS {table_name};\"\n",
    "    cursor.execute(drop_table_query)\n",
    "    # Create the table\n",
    "    create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(f\"{col} TEXT\" for col in columns)});\"\n",
    "    cursor.execute(create_table_query)\n",
    "    print(f\"Table {table_name} created\")\n",
    "\n",
    "    # Use PostgreSQL's COPY command to import data from the PSV file into the table\n",
    "    BAT_COMMANDS.append(f'echo \"Creating table {table_name}\"')\n",
    "    bat_command = f\"psql -d gnaf -U postgres -c \\\"\\\\copy {table_name} FROM '{psv_file_path}' DELIMITER '|' CSV HEADER\\\"\"\n",
    "    BAT_COMMANDS.append(bat_command)\n",
    "    BAT_COMMANDS.append(f'echo \"Table {table_name} created and populated with data from {psv_file_path}\"')\n",
    "    # subprocess.run(['psql', '-d', 'gnaf', '-U', 'postgres', '-c', f\"COPY {table_name} FROM '{psv_file_path}' DELIMITER '|' CSV HEADER\"], check=True)\n",
    "    # print(f\"Table {table_name} created and populated with data from {psv_file_path}\")\n",
    "with open('local-import_data.bat', 'w') as f:\n",
    "    f.write('\\n'.join(BAT_COMMANDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query('SELECT * FROM vic_address_detail LIMIT 100;', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_address(data):\n",
    "    address_parts = []\n",
    "\n",
    "    # Building name\n",
    "    if data['building_name']:\n",
    "        address_parts.append(data['building_name'])\n",
    "\n",
    "    # Lot number\n",
    "    lot_number = data['lot_number_prefix'] + data['lot_number'] + data['lot_number_suffix']\n",
    "    if lot_number.strip():\n",
    "        address_parts.append(lot_number)\n",
    "\n",
    "    # Flat details\n",
    "    flat_details = data['flat_type_code'] + data['flat_number_prefix'] + data['flat_number'] + data['flat_number_suffix']\n",
    "    if flat_details.strip():\n",
    "        address_parts.append(flat_details)\n",
    "\n",
    "    # Level details\n",
    "    level_details = data['level_type_code'] + data['level_number_prefix'] + data['level_number'] + data['level_number_suffix']\n",
    "    if level_details.strip():\n",
    "        address_parts.append(level_details)\n",
    "\n",
    "    # Number range\n",
    "    number_range = data['number_first_prefix'] + data['number_first'] + data['number_first_suffix'] + \"-\" + \\\n",
    "                   data['number_last_prefix'] + data['number_last'] + data['number_last_suffix']\n",
    "    if number_range.strip():\n",
    "        address_parts.append(number_range)\n",
    "\n",
    "    # Street, Locality/Suburb, Postcode, State\n",
    "    address_parts.extend([data['street'], data['locality'], data['postcode'], data['state']])\n",
    "\n",
    "    # Join all parts to form the address\n",
    "    address = ', '.join(filter(None, address_parts))\n",
    "\n",
    "    return address\n",
    "\n",
    "# Example data dictionary\n",
    "data = {\n",
    "    'building_name': 'Example Building',\n",
    "    'lot_number_prefix': '',\n",
    "    'lot_number': '10',\n",
    "    'lot_number_suffix': '',\n",
    "    'flat_type_code': 'APT',\n",
    "    'flat_number_prefix': '',\n",
    "    'flat_number': '20',\n",
    "    'flat_number_suffix': '',\n",
    "    'level_type_code': '',\n",
    "    'level_number_prefix': '',\n",
    "    'level_number': '',\n",
    "    'level_number_suffix': '',\n",
    "    'number_first_prefix': '',\n",
    "    'number_first': '',\n",
    "    'number_first_suffix': '',\n",
    "    'number_last_prefix': '',\n",
    "    'number_last': '',\n",
    "    'number_last_suffix': '',\n",
    "    'street': 'Example Street',\n",
    "    'locality': 'Example Suburb',\n",
    "    'postcode': '1234',\n",
    "    'state': 'Example State'\n",
    "}\n",
    "\n",
    "print(construct_address(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a connection to the database\n",
    "engine = create_engine('postgresql://postgres:postgres@localhost:5432/gnaf')\n",
    "conn_alchemy = engine.connect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_alchemy.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lot_number : str = ''\n",
    "flat_number : str = ''\n",
    "# number_first : str = '2A\\';--'\n",
    "number_first : str = '2A'\n",
    "number_last : str = ''\n",
    "street_name : str = 'East'\n",
    "street_type_code : str = 'St'\n",
    "street_suffix_code : str = ''\n",
    "locality_name : str = ''\n",
    "postcode : str = ''\n",
    "\n",
    "# Clean the input data to avoid SQL injection\n",
    "def sanitize_input(input_string : str):\n",
    "    return input_string.replace(\"'\", \"''\")\n",
    "\n",
    "locality_name = sanitize_input(locality_name)\n",
    "street_name = sanitize_input(street_name)\n",
    "street_type_code = sanitize_input(street_type_code)\n",
    "street_suffix_code = sanitize_input(street_suffix_code)\n",
    "number_first = sanitize_input(number_first)\n",
    "number_last = sanitize_input(number_last)\n",
    "lot_number = sanitize_input(lot_number)\n",
    "flat_number = sanitize_input(flat_number)\n",
    "postcode = sanitize_input(postcode)\n",
    "\n",
    "\n",
    "# Construct the SQL query\n",
    "\n",
    "params = []\n",
    "\n",
    "if locality_name and len(locality_name.strip()) > 0:\n",
    "    locality_name_sql = \"locality_name ILIKE %s\"\n",
    "    params.append(locality_name)\n",
    "else:\n",
    "    locality_name_sql = None\n",
    "\n",
    "if street_name and len(street_name.strip()) > 0:\n",
    "    street_name_sql = \"street_name ILIKE %s\" \n",
    "    params.append(street_name)\n",
    "else:\n",
    "    street_name_sql = None\n",
    "\n",
    "if street_type_code and len(street_type_code.strip()) > 0:\n",
    "    street_type_code_sql = \"street_type_code ILIKE %s\" \n",
    "    params.append(street_type_code)\n",
    "else:\n",
    "    street_type_code_sql = None\n",
    "\n",
    "if street_suffix_code and len(street_suffix_code.strip()) > 0:\n",
    "    street_suffix_code_sql = \"street_suffix_code ILIKE %s\" \n",
    "    params.append(street_suffix_code)\n",
    "else:\n",
    "    street_suffix_code_sql = None\n",
    "\n",
    "if number_first and len(number_first.strip()) > 0:\n",
    "    number_first_sql = \"CONCAT(number_first_prefix, number_first, number_first_suffix) ILIKE %s\"\n",
    "    params.append(number_first)\n",
    "else:\n",
    "    number_first_sql = None\n",
    "\n",
    "if number_last and len(number_last.strip()) > 0:\n",
    "    number_last_sql = \"CONCAT(number_last_prefix, number_last, number_last_suffix) ILIKE %s\"\n",
    "    params.append(number_last)\n",
    "else:\n",
    "    number_last_sql = None\n",
    "\n",
    "if lot_number and len(lot_number.strip()) > 0:\n",
    "    lot_number_sql = \"CONCAT(lot_number_prefix, lot_number, lot_number_suffix) ILIKE %s\"\n",
    "    params.append(lot_number)\n",
    "else:\n",
    "    lot_number_sql = None\n",
    "\n",
    "if flat_number and len(flat_number.strip()) > 0:\n",
    "    flat_number_sql = \"CONCAT(flat_number_prefix, flat_number, flat_number_suffix) ILIKE %s\"\n",
    "    params.append(flat_number)\n",
    "else:\n",
    "    flat_number_sql = None\n",
    "\n",
    "if postcode and len(postcode.strip()) > 0:\n",
    "    postcode_sql = \"postcode ILIKE %s\" \n",
    "    params.append(postcode)\n",
    "else:\n",
    "    postcode_sql = None\n",
    "\n",
    "table_locality_sql_list = list(filter(None, [locality_name_sql]))\n",
    "table_street_locality_sql_list = list(filter(None, [street_name_sql, street_type_code_sql, street_suffix_code_sql]))\n",
    "table_address_sql_list = list(filter(None, [number_first_sql, number_last_sql, lot_number_sql, flat_number_sql, postcode_sql]))\n",
    "\n",
    "table_locality_sql = \" WHERE (\" + \") AND (\".join(table_locality_sql_list) + \")\" if len(table_locality_sql_list) > 0 else \"\"\n",
    "table_street_locality_sql = \" WHERE (\" + \") AND (\".join(table_street_locality_sql_list) + \")\" if len(table_street_locality_sql_list) > 0 else \"\"\n",
    "table_address_sql = \" WHERE (\" + \") AND (\".join(table_address_sql_list) + \")\" if len(table_address_sql_list) > 0 else \"\"\n",
    "\n",
    "# street_name_sql = \"street_name ILIKE %s\" if street_name and len(street_name.strip()) > 0 else None\n",
    "# street_type_code_sql = \"street_type_code ILIKE %s\" if street_type_code and len(street_type_code.strip()) > 0 else None\n",
    "# street_suffix_code_sql = \"street_suffix_code ILIKE %s\" if street_suffix_code and len(street_suffix_code.strip()) > 0 else None\n",
    "\n",
    "# number_first_sql = \"number_first ILIKE %s\" if number_first and len(number_first.strip()) > 0 else None\n",
    "# number_last_sql = \"number_last ILIKE %s\" if number_last and len(number_last.strip()) > 0 else None\n",
    "# lot_number_sql = \"lot_number ILIKE %s\" if lot_number and len(lot_number.strip()) > 0 else None\n",
    "# flat_number_sql = \"flat_number ILIKE %s\" if flat_number and len(flat_number.strip()) > 0 else None\n",
    "# postcode_sql = \"postcode ILIKE %s\" if postcode and len(postcode.strip()) > 0 else None\n",
    "\n",
    "params = [f\"%{param}%\" for param in params]\n",
    "sql_query = f\"\"\"\n",
    "SELECT address_detail_pid, \n",
    "s.street_locality_pid AS street_locality_pid,\n",
    "l.locality_pid AS locality_pid,\n",
    "state_pid, \n",
    "building_name, \n",
    "lot_number_prefix, lot_number, lot_number_suffix, \n",
    "flat_type_code, \n",
    "flat_number_prefix , flat_number , flat_number_suffix, \n",
    "level_type_code, \n",
    "level_number_prefix, level_number, level_number_suffix, \n",
    "number_first_prefix, number_first, number_first_suffix, \n",
    "number_last_prefix, number_last, number_last_suffix, \n",
    "street_name, street_type_code, street_suffix_code, \n",
    "locality_name, \n",
    "postcode \n",
    "FROM (SELECT * FROM vic_locality {table_locality_sql}) AS l \n",
    "JOIN (SELECT * FROM vic_street_locality {table_street_locality_sql}) \n",
    "AS s ON l.locality_pid = s.locality_pid \n",
    "JOIN (SELECT * FROM vic_address_detail {table_address_sql}) \n",
    "AS a ON a.street_locality_pid = s.street_locality_pid\"\"\"\n",
    "\n",
    "assert sql_query.count('%s') == len(params)\n",
    "cursor.execute(sql_query, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = cursor.fetchall()\n",
    "columns = [desc[0] for desc in cursor.description]\n",
    "df = pd.DataFrame(rows, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_alchemy.rollback()\n",
    "pd.read_sql_query(text(\"\"\"\n",
    "SELECT\n",
    "address_detail_pid,\n",
    "building_name, \n",
    "CONCAT(lot_number_prefix, lot_number, lot_number_suffix) AS lot_number,\n",
    "flat_type_code, \n",
    "CONCAT(flat_number_prefix, flat_number, flat_number_suffix) AS flat_number,\n",
    "level_type_code, \n",
    "CONCAT(level_number_prefix, level_number, level_number_suffix) AS level_number,\n",
    "CONCAT(number_first_prefix, number_first, number_first_suffix) AS number_first,\n",
    "CONCAT(number_last_prefix, number_last, number_last_suffix) AS number_last,\n",
    "street_name, street_type_code, street_suffix_code, \n",
    "locality_name, state_pid, postcode\n",
    "FROM (SELECT * FROM vic_locality \n",
    ") AS l \n",
    "JOIN (SELECT * FROM vic_street_locality \n",
    ") AS s ON l.locality_pid = s.locality_pid\n",
    "JOIN (SELECT * FROM vic_address_detail \n",
    "WHERE flat_number IS NOT NULL AND lot_number IS NOT NULL\n",
    ") AS a ON a.street_locality_pid = s.street_locality_pid\n",
    "LIMIT 10\n",
    ";\"\"\"), conn_alchemy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT DISTINCT(level_type_code) FROM vic_address_detail;\")\n",
    "result = cursor.fetchall()\n",
    "df = pd.DataFrame(result, columns=[desc[0] for desc in cursor.description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'vic_address_detail';\")\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT * FROM authority_code_level_type_aut\")\n",
    "result = cursor.fetchall()\n",
    "columns = [desc[0] for desc in cursor.description]\n",
    "pd.DataFrame(result, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME LIKE 'authority%';\")\n",
    "result = cursor.fetchall()\n",
    "columns = [desc[0] for desc in cursor.description]\n",
    "pd.DataFrame(result, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" || ', ' || \".join(columns[4:22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('SELECT * FROM vic_street_locality LIMIT 10;', conn_alchemy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"SELECT * FROM (SELECT * FROM vic_locality WHERE locality_name ILIKE %s) AS localities INNER JOIN vic_address_detail AS addressesON localities.locality_pid = addresses.locality_pid) LIMIT 100;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('SELECT * FROM vic_locality WHERE locality_name ILIKE \\'%Ascot Vale%\\';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of  all columns in the table address_main\n",
    "cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'address';\")\n",
    "columns = cursor.fetchall()\n",
    "cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'address_1';\")\n",
    "columns_1 = cursor.fetchall()\n",
    "cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'address_main';\")\n",
    "columns_main = cursor.fetchall()\n",
    "cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'address_sup';\")\n",
    "columns_sup = cursor.fetchall()\n",
    "columns = [i[0] for i in columns]\n",
    "columns_1 = [i[0] for i in columns_1]\n",
    "columns_main = [i[0] for i in columns_main]\n",
    "columns_sup = [i[0] for i in columns_sup]\n",
    "\n",
    "assert set(columns) == set(columns_1)\n",
    "assert len(set(columns_sup) - set(columns)) == 0\n",
    "\n",
    "set(columns_sup) & set(columns_main)\n",
    "\n",
    "assert len({'gid', 'pfi', 'ufi', 'ufi_cr', 'ufi_old'} - set(columns)) == 0\n",
    "\n",
    "assert len(columns_main) + len(columns_sup) - len({'gid', 'pfi', 'ufi', 'ufi_cr', 'ufi_old'}) + 1 == len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that count of rows in address_main is equal to count of rows in address_sup, and sum of count of rows in address and address_1 is equal to count of rows in address_main\n",
    "cursor.execute(\"SELECT COUNT(*) FROM address_main;\")\n",
    "count_main = cursor.fetchall()\n",
    "cursor.execute(\"SELECT COUNT(*) FROM address_sup;\")\n",
    "count_sup = cursor.fetchall()\n",
    "cursor.execute(\"SELECT COUNT(*) FROM address;\")\n",
    "count = cursor.fetchall()\n",
    "cursor.execute(\"SELECT COUNT(*) FROM address_1;\")\n",
    "count_1 = cursor.fetchall()\n",
    "count_main = count_main[0][0]\n",
    "count_sup = count_sup[0][0]\n",
    "count = count[0][0]\n",
    "count_1 = count_1[0][0]\n",
    "assert count_main == count_sup\n",
    "assert count + count_1 == count_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that ufi in address_main is unique\n",
    "cursor.execute(\"SELECT ufi FROM address_main;\")\n",
    "ufi_main = cursor.fetchall()\n",
    "cursor.execute(\"SELECT DISTINCT(ufi) FROM address_main;\")\n",
    "ufi_main_distinct = cursor.fetchall()\n",
    "ufi_main = [int(i[0]) for i in ufi_main]\n",
    "ufi_main_distinct = [int(i[0]) for i in ufi_main_distinct]\n",
    "# 45s - 1min\n",
    "assert len(ufi_main) == len(ufi_main_distinct)\n",
    "# Assert that ufi in address_sup is unique\n",
    "cursor.execute(\"SELECT ufi FROM address_main;\")\n",
    "ufi_sup = cursor.fetchall()\n",
    "cursor.execute(\"SELECT DISTINCT(ufi) FROM address_sup;\")\n",
    "ufi_sup_distinct = cursor.fetchall()\n",
    "ufi_sup = [int(i[0]) for i in ufi_sup]\n",
    "ufi_sup_distinct = [int(i[0]) for i in ufi_sup_distinct]\n",
    "# 45s - 1min\n",
    "assert len(ufi_sup) == len(ufi_sup_distinct)\n",
    "\n",
    "# Assert that ufi in address_sup is unique\n",
    "cursor.execute(\"SELECT ufi FROM address;\")\n",
    "ufi = cursor.fetchall()\n",
    "cursor.execute(\"SELECT DISTINCT(ufi) FROM address;\")\n",
    "ufi_distinct = cursor.fetchall()\n",
    "ufi = [int(i[0]) for i in ufi]\n",
    "ufi_distinct = [int(i[0]) for i in ufi_distinct]\n",
    "# 45s - 1min\n",
    "assert len(ufi) == len(ufi_distinct)\n",
    "\n",
    "# Assert that ufi in address_sup is unique\n",
    "cursor.execute(\"SELECT ufi FROM address_1;\")\n",
    "ufi_1 = cursor.fetchall()\n",
    "cursor.execute(\"SELECT DISTINCT(ufi) FROM address_1;\")\n",
    "ufi_1_distinct = cursor.fetchall()\n",
    "ufi_1 = [int(i[0]) for i in ufi_1]\n",
    "ufi_1_distinct = [int(i[0]) for i in ufi_1_distinct]\n",
    "# 45s - 1min\n",
    "assert len(ufi_1) == len(ufi_1_distinct)\n",
    "\n",
    "assert len(set(ufi) & set(ufi_1)) == 0\n",
    "assert set(ufi_main) == set(ufi_sup)\n",
    "assert set(ufi_main) == set(ufi + ufi_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = cursor.fetchone()[0]\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(DISTINCT ufi) FROM public.tr_road_all;\")\n",
    "unique_values = cursor.fetchone()[0]\n",
    "\n",
    "print(\"Total rows: \", total_rows)   \n",
    "print(\"Unique values: \", unique_values)\n",
    "\n",
    "assert total_rows == unique_values, \"The 'ufi' column is not unique\"\n",
    "\n",
    "# Check if the 'ufi' column is unique\n",
    "cursor.execute(\"SELECT COUNT(ufi) FROM public.tr_road;\")\n",
    "total_rows = cursor.fetchone()[0]\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(DISTINCT ufi) FROM public.tr_road;\")\n",
    "unique_values = cursor.fetchone()[0]\n",
    "\n",
    "print(\"Total rows: \", total_rows)   \n",
    "print(\"Unique values: \", unique_values)\n",
    "\n",
    "assert total_rows == unique_values, \"The 'ufi' column is not unique\"\n",
    "\n",
    "\n",
    "# Assert that the columns in both tables are the same\n",
    "cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'tr_road_all';\")\n",
    "columns_tr_road_all = cursor.fetchall()\n",
    "cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'tr_road';\")\n",
    "columns_tr_road = cursor.fetchall()\n",
    "assert set(columns_tr_road_all) == set(columns_tr_road), \"The columns among tables are different\"\n",
    "\n",
    "# Assert that tr_road_all  = tr_road + paper_roads\n",
    "cursor.execute(\"SELECT ufi FROM tr_road_all;\")\n",
    "ufi_tr_road_all_list = cursor.fetchall()\n",
    "cursor.execute(\"SELECT ufi FROM tr_road;\")\n",
    "ufi_tr_road_list = cursor.fetchall()\n",
    "\n",
    "ufi_tr_road_list = [int(i[0]) for i in ufi_tr_road_list]\n",
    "ufi_tr_road_all_list = [int(i[0]) for i in ufi_tr_road_all_list]\n",
    "\n",
    "assert len(set(ufi_tr_road_list) - set(ufi_tr_road_all_list)) == 0\n",
    "\n",
    "cursor.execute(\"SELECT ufi FROM public.tr_road_all WHERE ftype_code = 'paper_road';\")\n",
    "paper_road_ufi_list = cursor.fetchall()\n",
    "paper_road_ufi_list = [int(i[0]) for i in paper_road_ufi_list]\n",
    "\n",
    "assert set(paper_road_ufi_list) == set(ufi_tr_road_all_list) - set(ufi_tr_road_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT * FROM public.tr_road_infrastructure LIMIT 100;\")\n",
    "results = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT COUNT(DISTINCT ufi) FROM public.tr_road_infrastructure;\")\n",
    "unique_ufi = cursor.fetchone()[0]\n",
    "cursor.execute(\"SELECT COUNT(ufi) FROM public.tr_road_infrastructure;\")\n",
    "total_ufi = cursor.fetchone()[0]\n",
    "assert unique_ufi == total_ufi, \"The 'ufi' column is not unique\"\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(DISTINCT ufi) FROM public.tr_road_infrastructure_all;\")\n",
    "unique_ufi_all = cursor.fetchone()[0]\n",
    "cursor.execute(\"SELECT COUNT(ufi) FROM public.tr_road_infrastructure_all;\")\n",
    "total_ufi_all = cursor.fetchone()[0]\n",
    "assert unique_ufi_all == total_ufi_all, \"The 'ufi' column is not unique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the columns in both tables are the same\n",
    "cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'tr_road_infrastructure_all';\")\n",
    "columns_tr_road_infrastructure_all = cursor.fetchall()\n",
    "cursor.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'tr_road_infrastructure';\")\n",
    "columns_tr_road_infrastructure = cursor.fetchall()\n",
    "assert set(columns_tr_road_infrastructure_all) == set(columns_tr_road_infrastructure), \"The columns among tables are different\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT ufi FROM public.tr_road_infrastructure;\")\n",
    "unique_ufi = cursor.fetchall()\n",
    "unique_ufi = [int(i[0]) for i in unique_ufi]\n",
    "\n",
    "cursor.execute(\"SELECT ufi FROM public.tr_road_infrastructure_all;\")\n",
    "unique_ufi_all = cursor.fetchall()\n",
    "unique_ufi_all = [int(i[0]) for i in unique_ufi_all]\n",
    "\n",
    "# assert set(unique_ufi_all) contains set(unique_ufi), \"The 'ufi' column is not unique\"\n",
    "assert len(set(unique_ufi) - set(unique_ufi_all)) == 0, \"The 'ufi' column is not unique\"\n",
    "\n",
    "# Select all paper nodes\n",
    "cursor.execute(\"SELECT ufi FROM public.tr_road_infrastructure_all WHERE ftype_code = 'paper_node';\")\n",
    "paper_nodes = cursor.fetchall()\n",
    "paper_nodes = [int(i[0]) for i in paper_nodes]\n",
    "assert set(unique_ufi_all) - set(unique_ufi) == set(paper_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_dbf('data/road/road.dbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASHARE_DIR = \"../local/datashare\"\n",
    "DIRNAMES = [\"PTV\", \"TRANSPORT\", \"VMADD\", \"VMPROP\", \"VMTRANS\"]\n",
    "DBTABLES = []\n",
    "for dirname in DIRNAMES:\n",
    "    dirpath = os.path.join(DATASHARE_DIR, dirname)\n",
    "    shp_files = [f.split(\".\")[0] for f in os.listdir(dirpath) if f.endswith('.shp')]\n",
    "    dbf_files = [f.split(\".\")[0] for f in os.listdir(dirpath) if f.endswith('.dbf')]\n",
    "    for filename in shp_files:\n",
    "        assert filename in dbf_files, f\"File {filename}.dbf not found in {dirpath}\"\n",
    "    for filename in dbf_files:\n",
    "        if filename in shp_files:\n",
    "            file_extension = \"shp\"\n",
    "        else:\n",
    "            file_extension = \"dbf\"\n",
    "        filepath = os.path.abspath(os.path.join(dirpath, f\"{filename}.{file_extension}\"))\n",
    "        database_name = dirname.lower()\n",
    "        table_name = filename.lower()\n",
    "        DBTABLES.append((database_name, table_name, file_extension, filepath))\n",
    "len(DBTABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona\n",
    "\n",
    "for database_name, table_name, file_extension, filepath in DBTABLES:\n",
    "    if file_extension == \"dbf\":\n",
    "        # Open the shapefile in read mode\n",
    "        with fiona.open(filepath, 'r') as shp:\n",
    "            # Get the count of features (lines) in the shapefile\n",
    "            num_features = len(shp)\n",
    "            print(\"Database: \", database_name)\n",
    "            print(\"Table: \", table_name)\n",
    "            print(\"Number of features:\", num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = \"postgres\"\n",
    "PASSWORD = \"postgres\"\n",
    "LOGDIR = \"../local/logs/db\"\n",
    "if not os.path.exists(LOGDIR):\n",
    "    os.makedirs(LOGDIR, exist_ok=True)\n",
    "else:\n",
    "    for file in os.listdir(LOGDIR):\n",
    "        os.remove(f\"{LOGDIR}/{file}\")\n",
    "with open(\"local-load-postgres.bat\", \"w\") as f:\n",
    "    f.write(\"@echo on\\n\")\n",
    "    # Set password\n",
    "    f.write(f'set PGPASSWORD={PASSWORD}\\n')\n",
    "    \n",
    "    for database_name, table_name, file_extension, filepath in DBTABLES:\n",
    "        log_filepath = os.path.abspath(f\"{LOGDIR}/{database_name}.{table_name}.log\")\n",
    "        if file_extension == \"dbf\":\n",
    "            command = f'shp2pgsql -I \"{filepath}\" public.{table_name} | psql -U {USER} -d \"{database_name}\" >> \"{log_filepath}\"'\n",
    "        # else:\n",
    "        #     command = f'shp2pgsql -s 7844 -I \"{filepath}\" public.{table_name} | psql -U {USER} -d \"{database_name}\" >> \"{log_filepath}\"'\n",
    "            f.write(command + \"\\n\")\n",
    "            command = f'echo \"Loaded {filepath} into database {database_name} table {table_name}.\"'\n",
    "            f.write(command + \"\\n\")\n",
    "            command = f'echo \"Loaded {filepath} into database {database_name} table {table_name}.\" >> \"{log_filepath}\"'\n",
    "            f.write(command + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, filedirs in SHP_FILEDIRS.items():\n",
    "    for filename, dirpath in filedirs.items():\n",
    "        dbf_path = os.path.join(dirpath, filename + \".dbf\")\n",
    "        shp_path = os.path.join(dirpath, filename + \".shp\")\n",
    "        assert os.path.exists(dbf_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a logger\n",
    "\n",
    "def create_logger(log_filepath, level=logging.INFO, name = \"root\"):\n",
    "\n",
    "    os.makedirs(os.path.dirname(log_filepath), exist_ok=True)\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # Create a handler for logging to file\n",
    "    formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "    file_handler = logging.FileHandler(log_filepath)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # Create a handler for logging to console\n",
    "    formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    logger = create_logger(\"../local/logs/load-postgres.log\")\n",
    "\n",
    "    DATASHARE_DIR = \"../local/datashare\"\n",
    "    SHP_DIRNAMES = [\"PTV\", \"TRANSPORT\", \"VMADD\", \"VMPROP\", \"VMTRANS\"]\n",
    "    SHP_DIRS = {dirname: os.path.join(DATASHARE_DIR, dirname) for dirname in SHP_DIRNAMES}\n",
    "    SHP_FILEDIRS = {\n",
    "        dirname: {\n",
    "            filename.split(\".\")[0]: dirpath\n",
    "            for filename in os.listdir(dirpath)\n",
    "            if filename.endswith(\".shp\")\n",
    "        }\n",
    "        for dirname, dirpath in SHP_DIRS.items()\n",
    "    }\n",
    "    user = \"postgres\"\n",
    "    password = \"postgres\"\n",
    "    with open(\"local-create-database.sql\", \"w\") as f:\n",
    "\n",
    "        for dirname, filedirs in SHP_FILEDIRS.items():\n",
    "            database_name = dirname.lower()\n",
    "\n",
    "            sql = f\"DROP DATABASE IF EXISTS {database_name};\"\n",
    "            f.write(sql + \"\\n\")\n",
    "            sql = f\"CREATE DATABASE {database_name} WITH OWNER = {user} ENCODING = 'UTF8' CONNECTION LIMIT = -1;\"\n",
    "            f.write(sql + \"\\n\")\n",
    "            # Use the database\n",
    "            sql = f\"\\\\c {database_name};\"\n",
    "            f.write(sql + \"\\n\")\n",
    "            sql = f\"CREATE EXTENSION IF NOT EXISTS postgis;\"\n",
    "            f.write(sql + \"\\n\")\n",
    "        \n",
    "\n",
    "\n",
    "    # conn = psycopg2.connect(\n",
    "    #     dbname=\"postgres\",\n",
    "    #     user=\"postgres\",\n",
    "    #     password=\"postgres\",\n",
    "    #     host=\"localhost\",\n",
    "    #     port=\"5432\",\n",
    "    # )\n",
    "\n",
    "    # conn.autocommit = True  # Set autocommit mode to True\n",
    "\n",
    "    # cursor = conn.cursor()\n",
    "    \n",
    "    # for dirname, filedirs in SHP_FILEDIRS.items():\n",
    "    #     database_name = dirname.lower()\n",
    "    #     # Drop the database if it exists\n",
    "    #     cursor.execute(f\"DROP DATABASE IF EXISTS {database_name};\")\n",
    "    #     logger.info(f\"[DROPPED] Dropped database {database_name}.\")\n",
    "\n",
    "    # for dirname, filedirs in SHP_FILEDIRS.items():\n",
    "    #     database_name = dirname.lower()\n",
    "    #     # Create a new database using psycopg2\n",
    "    #     cursor.execute(\n",
    "    #         f\"CREATE DATABASE {database_name} WITH OWNER = {user} ENCODING = 'UTF8' CONNECTION LIMIT = -1;\"\n",
    "    #     )\n",
    "    #     logger.info(f\"[CREATED] Created database {database_name}.\")\n",
    "    #     # Create extensions\n",
    "    #     cursor.execute(\"CREATE EXTENSION IF NOT EXISTS postgis;\")\n",
    "    #     logger.info(f\"[CREATED] Created postgis extension in database {database_name}.\")\n",
    "\n",
    "    # conn.commit()\n",
    "\n",
    "    # cursor.close()\n",
    "    # conn.close()\n",
    "\n",
    "    if not os.path.exists(\"../local/logs/db\"):\n",
    "        os.makedirs(f\"../local/logs/db\", exist_ok=True)\n",
    "    else:\n",
    "        for file in os.listdir(\"../local/logs/db\"):\n",
    "            os.remove(f\"../local/logs/db/{file}\")\n",
    "\n",
    "    # Create a batch file to load the shapefiles into the database\n",
    "    with open(\"local-load-postgres.bat\", \"w\") as f:\n",
    "        f.write(\"@echo on\\n\")\n",
    "        # Set password\n",
    "        f.write(f'set PGPASSWORD={password}\\n')\n",
    "        \n",
    "        f.write('psql -U postgres -a -f \"local-create-database.sql\"\\n')\n",
    "        '''\n",
    "        psql \"postgresql://postgres:postgres@localhost:5432/postgres\" -a -f \"local-create-database.sql\"\n",
    "        '''\n",
    "        for dirname, filedirs in SHP_FILEDIRS.items():\n",
    "            database_name = dirname.lower()\n",
    "            for shp_name, shp_filedir in filedirs.items():\n",
    "                shp_filepath = os.path.join(shp_filedir, f\"{shp_name}.shp\")\n",
    "                shp_filepath = os.path.abspath(shp_filepath)\n",
    "                table_name = shp_name.lower()\n",
    "                log_filepath = f\"../local/logs/db/{database_name}_{table_name}.log\"\n",
    "                log_filepath = os.path.abspath(log_filepath)\n",
    "                command = f'shp2pgsql -s 7844 -I \"{shp_filepath}\" public.{table_name} | psql \"postgresql://{user}:{password}@localhost:5432/{database_name}\" >> \"{log_filepath}\"'\n",
    "                f.write(command + \"\\n\")\n",
    "                command = f'echo \"Loaded {shp_name}.shp into database {database_name} table {table_name}.\"'\n",
    "                f.write(command + \"\\n\")\n",
    "                command = f'echo \"Loaded {shp_name}.shp into database {database_name} table {table_name}.\" >> \"{log_filepath}\"'\n",
    "                f.write(command + \"\\n\")\n",
    "\n",
    "        f.write(\"pause\")\n",
    "\n",
    "    # Execute the batch file\n",
    "    # result = subprocess.call(\"local-load-postgres.bat\", shell=True)\n",
    "\n",
    "    # for dirname, filedirs in SHP_FILEDIRS.items():\n",
    "    #     database_name = dirname\n",
    "    #     for shp_name, shp_filedir in filedirs.items():\n",
    "    #         # for shp_name, shp_dirpath in shp_path_info.items():\n",
    "    #         shp_filepath = os.path.join(shp_filedir, f\"{shp_name}.shp\")\n",
    "    #         shp_filepath = os.path.abspath(shp_filepath)\n",
    "    #         table_name = shp_name\n",
    "    #         log_filepath = f\"../local/logs/db/{database_name}_{table_name}.log\"\n",
    "    #         log_filepath = os.path.abspath(log_filepath)\n",
    "    #         command = f'shp2pgsql -s 7844 -I {shp_filepath} {table_name} | psql \"postgresql://{user}:{password}@localhost:5432/{database_name}\" >> \"{log_filepath}\"'\n",
    "\n",
    "\n",
    "            # try:\n",
    "            #     # Start the row count tracking thread in a background process\n",
    "            #     # thread = threading.Thread(target=track_row_count, args=(f\"{database_name}_{table_name}.log\",))\n",
    "            #     # thread.start()\n",
    "            #     # Execute the commands and also capture the output in the console\n",
    "            #     result = subprocess.call(command, shell=True)\n",
    "                \n",
    "            #     # result = subprocess.run(\n",
    "            #     #     command,\n",
    "            #     #     shell=True,\n",
    "            #     #     stdout=subprocess.PIPE,\n",
    "            #     #     stderr=subprocess.STDOUT,\n",
    "            #     #     text=True,\n",
    "            #     # )\n",
    "                \n",
    "            #     # with open(f\"../local/logs/db/{database_name}_{table_name}.log\", \"a\") as f:\n",
    "            #     #     f.write(result.stdout.decode(\"utf-8\"))\n",
    "            #     logger.info(f\"[FINISHED] Loaded {shp_name}.shp into database {database_name} table {table_name}.\")\n",
    "\n",
    "            #     # # Troubleshooting\n",
    "            #     # # Count the number of rows in the table\n",
    "            #     # for _ in range(5):\n",
    "            #     #     try:\n",
    "            #     #         conn = psycopg2.connect(\n",
    "            #     #             dbname=database_name,\n",
    "            #     #             user=user,\n",
    "            #     #             password=password,\n",
    "            #     #             host=\"localhost\",\n",
    "            #     #             port=\"5432\",\n",
    "            #     #         )\n",
    "            #     #         cursor = conn.cursor()\n",
    "            #     #         cursor.execute(f\"SELECT COUNT(*) FROM {table_name};\")\n",
    "            #     #         count = cursor.fetchone()[0]\n",
    "            #     #         logger.info(f\"Number of rows in {table_name}: {count}\")\n",
    "            #     #         cursor.close()\n",
    "            #     #         conn.close()\n",
    "            #     #     except Exception as e:\n",
    "            #     #         logger.error(f\"Error counting rows in {table_name}.\")\n",
    "            #     #         logger.error(e)\n",
    "            #     #         continue\n",
    "                    \n",
    "            #     #     time.sleep(5)\n",
    "\n",
    "                \n",
    "            # except subprocess.CalledProcessError as e:\n",
    "            #     logger.error(f\"Error loading {shp_name}.shp into database {database_name} table {table_name}.\")\n",
    "            #     logger.error(e.output)\n",
    "            #     continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
