{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "\n",
    "# Database connection details\n",
    "db_user = \"postgres\"\n",
    "db_password = \"postgres\"\n",
    "db_host = \"localhost\"\n",
    "db_port = 5432\n",
    "db_name = \"gtfs\"  # Replace with the actual database name\n",
    "\n",
    "# Create the database URL\n",
    "db_url = f\"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "\n",
    "# Create an SQLAlchemy engine to connect to the database\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Now you have an active database connection through SQLAlchemy\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    host=db_host,\n",
    "    port=db_port,\n",
    "    database=db_name,\n",
    "    user=db_user,\n",
    "    password=db_password\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# # Create table\n",
    "# cursor.execute(\n",
    "#     \"CREATE TABLE IF NOT EXISTS tweets (id SERIAL PRIMARY KEY, tweet VARCHAR(255), sentiment VARCHAR(255))\"\n",
    "# )\n",
    "\n",
    "# conn.commit()\n",
    "\n",
    "# cursor.close()\n",
    "# conn.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # URL of the main GTFS ZIP file\n",
    "# url = \"http://data.ptv.vic.gov.au/downloads/gtfs.zip\"\n",
    "# https://discover.data.vic.gov.au/dataset/timetable-and-geographic-information-gtfs\n",
    "# https://data.ptv.vic.gov.au/downloads/GTFSReleaseNotes.pdf\n",
    "\n",
    "# GTFS Release\n",
    "# The DTP GTFS data has been exported by operational branches listed in the folder numbers below:\n",
    "# 1 - Regional Train\n",
    "# 2 - Metropolitan Train\n",
    "# 3 - Metropolitan Tram\n",
    "# 4 - Metropolitan Bus\n",
    "# 5 - Regional Coach\n",
    "# 6 - Regional Bus\n",
    "# 7 - TeleBus\n",
    "# 8 â€“ Night Bus\n",
    "# 10 - Interstate\n",
    "# 11 - SkyBus\n",
    "# The GTFS data provided for each of the 10 Operational branches is in the form of 8 files and is\n",
    "# described in the following table: \n",
    "\n",
    "\n",
    "\n",
    "def process_google_transit_from_zipfile_object(google_transit_zip_ref):\n",
    "    \n",
    "    # Create a dictionary to store DataFrames for each folder\n",
    "    google_transit_data = {}\n",
    "\n",
    "    nested_file_list = google_transit_zip_ref.namelist()\n",
    "    for nested_file_name in nested_file_list:\n",
    "        if nested_file_name.endswith('.txt'):\n",
    "            with google_transit_zip_ref.open(nested_file_name) as nested_file:\n",
    "                # Read the CSV content as a Pandas DataFrame\n",
    "                google_transit_data[nested_file_name.removesuffix('.txt')] = pd.read_csv(nested_file, keep_default_na=False)\n",
    "\n",
    "    return pd.Series(google_transit_data)\n",
    "\n",
    "def process_gtfs_from_zipfile_object(main_zip_ref):\n",
    "    # Create a dictionary to store all data\n",
    "    all_data = {}\n",
    "\n",
    "    # Iterate through the file list in the main GTFS ZIP\n",
    "    for file_name in main_zip_ref.namelist():\n",
    "        # Check if the item is a directory\n",
    "        if file_name.endswith('/'):\n",
    "            subdir_name = file_name.strip('/')\n",
    "            \n",
    "            # Look for the nested ZIP file inside the subdirectory\n",
    "            nested_zip_path = f\"{subdir_name}/google_transit.zip\"\n",
    "            \n",
    "            # Check if the nested ZIP file exists in the subdirectory\n",
    "            if nested_zip_path in main_zip_ref.namelist():\n",
    "                \n",
    "                # Extract the nested ZIP contents directly from memory\n",
    "                with main_zip_ref.open(nested_zip_path) as nested_zip_file:\n",
    "                    with zipfile.ZipFile(io.BytesIO(nested_zip_file.read())) as nested_zip_ref:\n",
    "                        google_transit_data = process_google_transit_from_zipfile_object(nested_zip_ref)\n",
    "                \n",
    "                all_data[int(subdir_name)] = google_transit_data\n",
    "\n",
    "            else:\n",
    "                print(\"Nested ZIP file not found in\", subdir_name)\n",
    "\n",
    "    # Convert the dictionary to a Pandas Series\n",
    "    all_data = pd.Series(all_data) \n",
    "\n",
    "    # Sort the series by the folder number\n",
    "    all_data.sort_index(inplace=True)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def process_google_transit_from_url(url):\n",
    "    # Send an HTTP GET request to get the main GTFS ZIP file content\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Create a ZipFile object from the response content\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as main_zip_ref:\n",
    "            return process_google_transit_from_zipfile_object(main_zip_ref)\n",
    "\n",
    "    else:\n",
    "        print(response, \"Failed to fetch the main GTFS ZIP file.\")\n",
    "\n",
    "def process_google_transit_from_local_zip(zip_path):\n",
    "    # Create a ZipFile object from the local ZIP file\n",
    "    with zipfile.ZipFile(zip_path) as main_zip_ref:\n",
    "        return process_google_transit_from_zipfile_object(main_zip_ref)\n",
    "\n",
    "def process_gtfs_from_url(url):\n",
    "    # Send an HTTP GET request to get the main GTFS ZIP file content\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Create a ZipFile object from the response content\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as main_zip_ref:\n",
    "            return process_gtfs_from_zipfile_object(main_zip_ref)\n",
    "\n",
    "    else:\n",
    "        print(response, \"Failed to fetch the main GTFS ZIP file.\")\n",
    "\n",
    "def process_gtfs_from_local_zip(zip_path):\n",
    "    # Create a ZipFile object from the local ZIP file\n",
    "    with zipfile.ZipFile(zip_path) as main_zip_ref:\n",
    "        return process_gtfs_from_zipfile_object(main_zip_ref)\n",
    "    \n",
    "\n",
    "data = pd.Series()\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk('downloads'):\n",
    "\n",
    "    for filename in filenames:\n",
    "        \n",
    "        gtfs_zip_file_path = os.path.join(dirpath, filename)\n",
    "        \n",
    "        gtfs_zip_parent_folder_name = gtfs_zip_file_path.split(os.sep)[-2]\n",
    "        \n",
    "        data[gtfs_zip_parent_folder_name] = process_gtfs_from_local_zip(gtfs_zip_file_path)\n",
    "\n",
    "# Get current time and date in format YYYYMMDD_HHMMSS\n",
    "current_time = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "data[current_time] = process_gtfs_from_url(\"http://data.ptv.vic.gov.au/downloads/gtfs.zip\")\n",
    "\n",
    "for version_id, dfx in data.items():\n",
    "    for branch_id, dfs in dfx.items():\n",
    "        # df = data['main'][service_id]\n",
    "        for df_name, df in dfs.items():\n",
    "            df['x_branch_id'] = branch_id\n",
    "            df['x_version_id'] = version_id\n",
    "            df.to_sql(df_name, engine, if_exists='append', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
